{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41034072-4b51-40ec-b16e-5c3b601a69b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ResNet Notebook\n",
    "\n",
    "This notebook is for reproducing the paper with ResNet. Biggest TODO: separate saving data between iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4054fe80-6332-4e58-a0c9-6338ceb6f191",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Dependencies\n",
    "\n",
    "Let's setup and configure dependencies. Note that since torchviz is used, graphviz needs to be specially installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ee8ce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import contextlib\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import random\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.io import ImageReadMode\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms as T\n",
    "from torchviz import make_dot # This requires external downloads as well\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b1d91ed-5a91-4f62-bc7f-31bfa59a65bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "def seed_worker(worker_id):\n",
    "    \"\"\"\n",
    "    Sets the seed of the worker to depend on the initial seed. Credit to\n",
    "    https://pytorch.org/docs/stable/notes/randomness.html\n",
    "    \"\"\"\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    \n",
    "torch.manual_seed(0)\n",
    "# For DataLoaders\n",
    "g = torch.Generator().manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d2045b5-74a7-41b8-b0be-2da5d6e6dfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare progress bar\n",
    "tqdm_variant = tqdm.tqdm # Assume not in Jupyter\n",
    "try:\n",
    "    # Credit https://stackoverflow.com/a/39662359\n",
    "    shell_name = get_ipython().__class__.__name__\n",
    "    if shell_name == \"ZMQInteractiveShell\":\n",
    "        # Case in Jupyter\n",
    "        tqdm_variant = tqdm.notebook.tqdm\n",
    "except NameError:\n",
    "    # Probably no iPhython instance, just use standard\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6e58de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11be810b-db52-466b-a73a-030dec6214b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Plotting and Debugging Tools\n",
    "\n",
    "Let's create some helpful functions for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e861da12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from Steven's ModelExamples\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.gca().get_xaxis().set_visible(False)\n",
    "    plt.gca().get_yaxis().set_visible(False)\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af008e45-5924-4d4b-b859-9b3701a50427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_batch_sample(loader):\n",
    "    dataiter = iter(loader)\n",
    "    imgs, labels = next(dataiter)\n",
    "    img_grid = torchvision.utils.make_grid(imgs)\n",
    "    imgs, labels = imgs.to(device), labels.to(device)\n",
    "    matplotlib_imshow(img_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57feb0c1-18ca-4346-a620-a26030a7877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cumulative_dist(\n",
    "    baseline_observations,\n",
    "    stabilized_observations\n",
    "):\n",
    "    \"\"\"Plot our reproduction of Figure 7.\"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(\"Cumulative Distributions\")\n",
    "    ax.set_xlabel(\"Feature Distance\")\n",
    "    ax.set_ylabel(\"Cumulative Fraction\")\n",
    "    ax.hist(\n",
    "        baseline_observations,\n",
    "        round(np.sqrt(len(baseline_observations))),\n",
    "        density=True,\n",
    "        histtype=\"step\",\n",
    "        cumulative=True,\n",
    "        label=\"Baseline\"\n",
    "    )\n",
    "    ax.hist(\n",
    "        stabilized_observations,\n",
    "        round(np.sqrt(len(stabilized_observations))),\n",
    "        density=True,\n",
    "        histtype=\"step\",\n",
    "        cumulative=True,\n",
    "        label=\"Stabilized\"\n",
    "    )\n",
    "    ax.legend()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17edab80-3391-4fd6-977d-635f89910893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_first_triplet(queries, positives, negatives):\n",
    "    a, b, c = queries[0], positives[0], negatives[0]\n",
    "    ToPIL = T.ToPILImage() # Converting function\n",
    "    fig, ax = plt.subplots(1, 3)\n",
    "    ax[0].imshow(ToPIL(a))\n",
    "    ax[1].imshow(ToPIL(b))\n",
    "    ax[2].imshow(ToPIL(c))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19f07a2d-02bf-426b-abe0-d4e319553a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_vs_quality(\n",
    "    qualities,\n",
    "    baseline_precision,\n",
    "    stabilized_precision\n",
    "):\n",
    "    \"\"\"Plots precision vs quality as in Figure 9.\"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(\"Precision vs Quality\")\n",
    "    ax.set_xlabel(\"JPEG Quality\")\n",
    "    ax.set_ylabel(\"Precision@top-1\")\n",
    "    plt.plot(qualities, baseline_precision, label=\"Baseline\")\n",
    "    plt.plot(qualities, stabilized_precision, label=\"Stabilized\")\n",
    "    ax.legend()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64bfaafa-c819-4b54-9609-11bdec012522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_distances(triplet_ranking_model, batch, item_idx=0):\n",
    "    with torch.no_grad():\n",
    "        output = triplet_ranking_model(*map(lambda x : x.to(device), batch))\n",
    "        qfeat = output[item_idx][0]\n",
    "        pfeat = output[item_idx][1]\n",
    "        nfeat = output[item_idx][2]\n",
    "        pos_dist = torch.sqrt(((qfeat - pfeat)**2).sum()).item()\n",
    "        neg_dist = torch.sqrt(((qfeat - nfeat)**2).sum()).item()\n",
    "        print(f\"Distance to positive: {pos_dist}, to negative: {neg_dist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc57be1-7ce4-4ee2-8a9d-cda6b6e0642d",
   "metadata": {},
   "source": [
    "This part is also helpful for storing results of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ab9f248-96b3-43bc-bf1a-7e45d4c33a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _def_save_filename(iteration_number : int, is_model : bool):\n",
    "    \"\"\"\n",
    "    Returns a save filename from an iteration number, and whether the\n",
    "    thing being saved is the model or the loss progress.\n",
    "    \"\"\"\n",
    "    # Make sure inputs are okay (no directory traversal attacks!)\n",
    "    if not isinstance(iteration_number, int):\n",
    "        raise TypeError(\"Iteration number should be an integer.\")\n",
    "\n",
    "    # Retrieve proper name\n",
    "    if is_model:\n",
    "        return os.path.join(\n",
    "            os.path.dirname(os.path.abspath(\"\")),\n",
    "            f\"model_save_{iteration_number}_{int(time.time())%1000000:06}.pt\"\n",
    "        )\n",
    "    return os.path.join(\n",
    "        os.path.dirname(os.path.abspath(\"\")),\n",
    "        f\"progress_save_{iteration_number}_{int(time.time())%1000000:06}.pt\"\n",
    "    )\n",
    "\n",
    "#\n",
    "# This class will make recording information easier\n",
    "#\n",
    "class TrainResult:\n",
    "    \"\"\"\n",
    "    Hold the results of a training round. Basically a nice wrapper.\n",
    "\n",
    "    Losses and accuracies should be averages over a batch for plot\n",
    "    labels to make sense.\n",
    "\n",
    "    Use whichever storage makes sense, but if one doesn't use the\n",
    "    storage then don't expect the corresponding plots to do anything.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        \"\"\"\n",
    "        Constructor - prepare local variables. Include model since the\n",
    "        one provided to trainer will likely be moved to a different\n",
    "        device. Provide the one that is actually trained here, and then\n",
    "        it can be used.\n",
    "        \"\"\"\n",
    "        self.train_loss_history = []\n",
    "        self.val_loss_history = []\n",
    "        self.train_accuracy_history = []\n",
    "        self.val_accuracy_history = []\n",
    "        self.time_training = 0.0\n",
    "        self.time_validating = 0.0\n",
    "        self.model = model\n",
    "\n",
    "    def plot_loss_train_valid_curves(self, ax, show_legend : bool=True):\n",
    "        \"\"\"Use matplotlib to show training curves. Supply axis in ax.\"\"\"\n",
    "        ax.plot(self.train_loss_history, label=\"Train\")\n",
    "        ax.plot(self.val_loss_history, label=\"Test\")\n",
    "        ax.set_title(f\"Model Loss\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        if show_legend:\n",
    "            ax.legend()\n",
    "\n",
    "    def plot_accuracy_train_valid_curves(self, ax, show_legend=True):\n",
    "        \"\"\"Use matplotlib to show training curves. Supply axis in ax.\"\"\"\n",
    "        ax.plot(self.train_accuracy_history, label=\"Train\")\n",
    "        ax.plot(self.val_accuracy_history, label=\"Test\")\n",
    "        ax.set_title(f\"Model Accuracy\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "        ax.set_ylabel(\"Accuracy\")\n",
    "        if show_legend:\n",
    "            ax.legend()\n",
    "\n",
    "    def print_time_info(self):\n",
    "        \"\"\"Prints some lines with information about timing.\"\"\"\n",
    "        print(f\"Spent {round(self.time_training)} seconds training.\")\n",
    "        print(f\"Spent {round(self.time_validating)} seconds evaluating.\")\n",
    "\n",
    "    def full_analysis(self, axs):\n",
    "        \"\"\"Display all analysis plots and print time information.\"\"\"\n",
    "        if len(self.train_loss_history) > 0:\n",
    "            self.plot_loss_train_valid_curves(axs[0])\n",
    "        if len(self.train_accuracy_history) > 0:\n",
    "            self.plot_accuracy_train_valid_curves(axs[1])\n",
    "        if self.time_training > 0.0:\n",
    "            self.print_time_info()\n",
    "\n",
    "    def save(self, save_filename_func, count : int=0):\n",
    "        \"\"\"\n",
    "        Uses the save_filename_func provided with the count to create\n",
    "        files that save the models current weights and the training\n",
    "        progress so far.\n",
    "\n",
    "        For count, 0 is by convention the final version.\n",
    "        \"\"\"\n",
    "        # Save model\n",
    "        torch.save(self.model.state_dict(), save_filename_func(count, True))\n",
    "        # Save progress\n",
    "        with open(save_filename_func(count, False), \"wb\") as file_obj:\n",
    "            pickle.dump(\n",
    "                {\n",
    "                    \"train_loss_history\" : self.train_loss_history,\n",
    "                    \"val_loss_history\" : self.val_loss_history,\n",
    "                    \"train_accuracy_history\" : self.train_accuracy_history,\n",
    "                    \"val_accuracy_history\" : self.val_accuracy_history,\n",
    "                    \"time_training\" : self.time_training,\n",
    "                    \"time_validating\" : self.time_validating\n",
    "                }, file_obj\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a7ec4f1-79b6-4814-b5d1-d705dfffd422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_sanity_display(model, input_iter):\n",
    "    \"\"\"\n",
    "    Show a computational graph for a model on device given an iterable\n",
    "    of inputs to provide to the model.\n",
    "    \"\"\"\n",
    "    res = model(*[item.to(device) for item in input_iter])\n",
    "    return make_dot(res, params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2bc7b5-3ab6-4314-adc5-78d3d976254e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Metrics\n",
    "\n",
    "Let's prepare some evaluation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90f9c48b-17ea-4c4e-967f-2c2b8e8e132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_get_accuracy(model_output, labels):\n",
    "    \"\"\"\n",
    "    Given model output and labels, finds the average accuracy over the \n",
    "    batch.\n",
    "    \"\"\"\n",
    "    preds = model_output.topk(1, dim=1)[1].t().flatten()\n",
    "    return ((preds == labels).sum() / len(preds)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e999aef-7d29-4c15-8ab4-5bdb530e9dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_classifier_get_accuracy(model_output, labels):\n",
    "    \"\"\"For stability trained classifier, finds the accuracy\"\"\"\n",
    "    return (\n",
    "        torch.sum(\n",
    "            model_output[0].topk(1, dim=1)[1].t().flatten()\n",
    "            == labels\n",
    "        )\n",
    "        / len(model_output[0])\n",
    "   ).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e451b31-983c-4cb5-9613-beb1aa72330e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Experiments\n",
    "\n",
    "Let's make some functions to run experiments on models.\n",
    "\n",
    "For the following, see how closely the triplet ranking models think compressed versions of the same images are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34f5e7fb-1544-4321-bba1-49c814249271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_observations(baseline_model, stabilized_model, imagefolder):\n",
    "    \"\"\"\n",
    "    Given a model trained for triplet ranking, make duplicate detection\n",
    "    observations.\n",
    "    \"\"\"\n",
    "    baseline_observations = []\n",
    "    stabilized_observations = []\n",
    "    \n",
    "    for img_name in os.listdir(imagefolder):\n",
    "        #\n",
    "        # Deal with images\n",
    "        #\n",
    "        original_img = Image.open(image_fpath)\n",
    "        # Thanks https://stackoverflow.com/a/30771751\n",
    "        buffer = StringIO.StringIO()\n",
    "        original_img.save(buffer, \"JPEG\", optimize=True, quality=50)\n",
    "        compressed_img = Image.open(buffer)\n",
    "        \n",
    "        orig_tensor = T.ToTensor(original_img)\n",
    "        comp_tensor = T.ToTensor(compressed_img)\n",
    "        #\n",
    "        # Make observations\n",
    "        #\n",
    "        baseline_model.eval()\n",
    "        stabilized_model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Baseline\n",
    "            original_rep = baseline_model(orig_tensor)\n",
    "            compressed_rep = baseline_model(comp_tensor)\n",
    "            dist = torch.sqrt(((original_rep - compressed_rep)**2).sum())\n",
    "            baseline_observations.append(dist)\n",
    "            # Stabilized\n",
    "            original_rep = stabilized_model(orig_tensor)\n",
    "            compressed_rep = stabilized_model(comp_tensor)\n",
    "            dist = torch.sqrt(((original_rep - compressed_rep)**2).sum())\n",
    "            stabilized_observations.append(dist)\n",
    "            \n",
    "    return baseline_observations, stabilized_observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671bb892-d181-4112-a463-914b8324e206",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Datasets\n",
    "\n",
    "Let's prepare some datasets. First, we'll make a custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cedeaca-3215-46cb-a183-ce1b8b1c2dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: an alternative to this paradigm is loading all the images using\n",
    "# the transform as a catalogue onto main memory, then detach on access\n",
    "# TODO - refactor\n",
    "class TripletRankingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 img_dir : str,\n",
    "                 pos_per_img : int,\n",
    "                 neg_per_pos : int,\n",
    "                 transform=None,\n",
    "                 training=True\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Point at directory which has subdirectories corresponding to\n",
    "        each class. Supply the number of positive images per image in\n",
    "        the subdirectories and the number of negative images to combine\n",
    "        with.\n",
    "        \"\"\"\n",
    "        if transform is None:\n",
    "            transform = nn.Identity()\n",
    "        self.transform = transform\n",
    "        self.mode = \"RGB\"#ImageReadMode.RGB\n",
    "        \n",
    "        #\n",
    "        # Initialize triplets\n",
    "        #\n",
    "        self.triplets = [ ]\n",
    "        \n",
    "        if training:\n",
    "            self._train_init(img_dir, pos_per_img, neg_per_pos)\n",
    "        else:\n",
    "            self._val_init(img_dir, pos_per_img, neg_per_pos)\n",
    "        \n",
    "    def _train_init(self, img_dir, pos_per_img, neg_per_pos):\n",
    "        # Generate randoms first, index into later. Need to count\n",
    "        # Iterate over classes\n",
    "        class_names = os.listdir(img_dir)\n",
    "        if os.path.exists(os.path.join(img_dir, class_names[0], \"images\")):\n",
    "            subdirectories_paths = list(map(\n",
    "                lambda x : os.path.join(img_dir, x, \"images\"),\n",
    "                class_names\n",
    "            ))\n",
    "        else:\n",
    "            subdirectories_paths = list(map(\n",
    "                lambda x : os.path.join(img_dir, x),\n",
    "                class_names\n",
    "            ))\n",
    "        for subdir_path in subdirectories_paths:\n",
    "            img_per_class = len(os.listdir(subdir_path))\n",
    "            break\n",
    "        randoms = np.random.randint(\n",
    "            0,\n",
    "            img_per_class,\n",
    "            (\n",
    "                len(subdirectories_paths),\n",
    "                img_per_class,\n",
    "                pos_per_img,\n",
    "                neg_per_pos + 1 # Last for positive selection\n",
    "            ),\n",
    "            dtype=\"int16\"\n",
    "        )\n",
    "        \n",
    "        # TODO - this method inefficient, only need to hold 2 numbers,\n",
    "        # some string format info\n",
    "        # DANGER - potential for loops with symbolic/hard links\n",
    "        # Iterate over classes\n",
    "        for subdir_idx, subdir_path in enumerate(subdirectories_paths):\n",
    "            exclude_subdir_idx =  [\n",
    "                            *range(subdir_idx),\n",
    "                            *range(\n",
    "                                subdir_idx + 1,\n",
    "                                len(subdirectories_paths)\n",
    "                            )\n",
    "                        ]\n",
    "            # Iterate over images\n",
    "            for img_idx, img in enumerate(os.listdir(subdir_path)):\n",
    "                img_path = os.path.join(subdir_path, img)\n",
    "                # Iterate over alike images\n",
    "                for pos_it in range(pos_per_img):\n",
    "                    pos_rand = randoms[subdir_idx][img_idx][pos_it][-1]\n",
    "                    # Adjust on equal\n",
    "                    if pos_rand == pos_it:\n",
    "                        if pos_rand == 0:\n",
    "                            pos_rand += 1\n",
    "                        else:\n",
    "                            pos_rand -= 1\n",
    "                    # Form name\n",
    "                    pos_path = os.path.join(\n",
    "                        subdir_path,\n",
    "                        f\"{class_names[subdir_idx]}_{pos_rand}.JPEG\"\n",
    "                    )\n",
    "                    # Need more randoms for next step\n",
    "                    neg_class_rands = np.random.choice(\n",
    "                        exclude_subdir_idx,\n",
    "                        neg_per_pos,\n",
    "                        replace=True\n",
    "                    )\n",
    "                    # Iterate over dissimilar images\n",
    "                    for neg_it in range(neg_per_pos):\n",
    "                        class_rand = neg_class_rands[neg_it]\n",
    "                        neg_class_path = subdirectories_paths[class_rand]\n",
    "                        neg_class = class_names[class_rand]\n",
    "                        neg_rand = randoms[subdir_idx][img_idx][pos_it][neg_it]\n",
    "                        # Form name\n",
    "                        neg_path = os.path.join(\n",
    "                            neg_class_path,\n",
    "                            f\"{neg_class}_{neg_rand}.JPEG\"\n",
    "                        )\n",
    "                        # Append\n",
    "                        self.triplets.append((img_path, pos_path, neg_path))\n",
    "                        \n",
    "    def _val_init(self, img_dir, pos_per_img, neg_per_pos):\n",
    "        # Generate randoms first, index into later. Need to count\n",
    "        # Iterate over classes\n",
    "        class_names = os.listdir(img_dir)\n",
    "        \n",
    "        if os.path.exists(os.path.join(img_dir, class_names[0], \"images\")):\n",
    "            subdirectories_paths = list(map(\n",
    "                lambda x : os.path.join(img_dir, x, \"images\"),\n",
    "                class_names\n",
    "            ))\n",
    "        else:\n",
    "            subdirectories_paths = list(map(\n",
    "                lambda x : os.path.join(img_dir, x),\n",
    "                class_names\n",
    "            ))\n",
    "            \n",
    "        img_per_class = dict()\n",
    "        img_names = []\n",
    "        for idx in range(len(class_names)):\n",
    "            subdir_path = subdirectories_paths[idx]\n",
    "            class_name = class_names[idx]\n",
    "            img_names.append(list(os.listdir(subdir_path)))\n",
    "            img_per_class[class_name] = len(img_names[-1])\n",
    "        \n",
    "        # TODO - this method inefficient, only need to hold 2 numbers,\n",
    "        # some string format info\n",
    "        # DANGER - potential for loops with symbolic/hard links\n",
    "        # Iterate over classes\n",
    "        for subdir_idx, subdir_path in enumerate(subdirectories_paths):\n",
    "            exclude_subdir_idx =  [\n",
    "                            *range(subdir_idx),\n",
    "                            *range(\n",
    "                                subdir_idx + 1,\n",
    "                                len(subdirectories_paths)\n",
    "                            )\n",
    "                        ]\n",
    "            this_dir = img_names[subdir_idx]\n",
    "            # Iterate over images\n",
    "            for img_idx, img in enumerate(this_dir):\n",
    "                img_path = os.path.join(subdir_path, img)\n",
    "                # Iterate over alike images\n",
    "                for pos_it in range(pos_per_img):\n",
    "                    # Form name\n",
    "                    rand_idx = np.random.choice([\n",
    "                        *range(pos_it),\n",
    "                        *range(pos_it + 1, img_per_class[class_names[pos_it]])\n",
    "                    ])\n",
    "                    pos_name = this_dir[rand_idx]\n",
    "                    # Form path\n",
    "                    pos_path = os.path.join(subdir_path, pos_name)\n",
    "                    # Need more randoms for next step\n",
    "                    neg_class_rands = np.random.choice(\n",
    "                        exclude_subdir_idx,\n",
    "                        neg_per_pos,\n",
    "                        replace=True\n",
    "                    )\n",
    "                    # Iterate over dissimilar images\n",
    "                    for neg_it in range(neg_per_pos):\n",
    "                        class_rand = neg_class_rands[neg_it]\n",
    "                        neg_class_path = subdirectories_paths[class_rand]\n",
    "                        neg_class = class_names[class_rand]\n",
    "                        # Select from the directory\n",
    "                        neg_name = np.random.choice(img_names[class_rand])\n",
    "                        # Form name\n",
    "                        neg_path = os.path.join(neg_class_path, neg_name)\n",
    "                        # Append\n",
    "                        self.triplets.append((img_path, pos_path, neg_path))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        this_triplet = self.triplets[idx]\n",
    "        #query = self.transform(read_image(this_triplet[0], self.mode))\n",
    "        #positive = self.transform(read_image(this_triplet[1], self.mode))\n",
    "        #negative = self.transform(read_image(this_triplet[2], self.mode))\n",
    "        query = self.transform(Image.open(this_triplet[0]).convert(self.mode))\n",
    "        positive = self.transform(Image.open(this_triplet[1]).convert(self.mode))\n",
    "        negative = self.transform(Image.open(this_triplet[2]).convert(self.mode))\n",
    "        return query, positive, negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3597933-c598-4edf-9d96-55a37a7ebf09",
   "metadata": {},
   "source": [
    "The following is helpful to keep around:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86997b7d-71ce-4e6f-9a5e-ef0725ee3271",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.path.abspath(\"\"), \"tiny-imagenet-200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28c8b26-39c0-4500-8b21-2c2b8f346b2a",
   "metadata": {},
   "source": [
    "We'll create dataset objects as we go since they take up memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e225f7-ffcc-44a3-8164-a3ba84e1f152",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Models\n",
    "\n",
    "We'll similarly create model objects as we go along, but this section contains things that simplify making them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ccb2b3-0eea-49dc-b080-9632eda8b6ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Helper Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "968864a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit https://discuss.pytorch.org/t/how-to-add-noise-to-mnist-dataset-when-using-pytorch/59745\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return (\n",
    "            tensor\n",
    "            + torch.empty(tensor.size(), device=device).normal_() * self.std\n",
    "            + self.mean\n",
    "        )\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            self.__class__.__name__\n",
    "            + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "366f9024-7be8-439c-8602-30c895b78433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies an amount of JPEG compression to a PIL image    \n",
    "class JPEGTransform(nn.Module):\n",
    "    def __init__(self, quality=50):\n",
    "        self.quality = quality\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        buffer = BytesIO()\n",
    "        img.save(buffer, \"JPEG\", optimize=True, quality=self.quality)\n",
    "        return Image.open(buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad20f6a0-d46f-4ee0-9539-27a914d4ab74",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01a52b46-8c49-4959-8f83-39a3851cad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletRanker(nn.Module):\n",
    "    \"\"\"Triplet ranking model.\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        \"\"\"Creates a triplet ranker out of the model.\"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, queries, positives, negatives):\n",
    "        \"\"\"Send batch of triplets through.\"\"\"\n",
    "        queries_encode = self.model(queries)\n",
    "        positives_encode = self.model(positives)\n",
    "        negatives_encode = self.model(negatives)\n",
    "        return queries_encode, positives_encode, negatives_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e18b078d-5644-4d88-8ca2-7037dd224c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StabilityTrainingModel(torch.nn.Module):\n",
    "    def __init__(self, other_model, perturbation=None):\n",
    "        \"\"\"\n",
    "        Wraps another model to allow for stability training. May specify\n",
    "        custom perturbation transformation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = other_model\n",
    "        \n",
    "        if perturbation is None:\n",
    "            self.perturb = AddGaussianNoise(0, 0.04)\n",
    "        else:\n",
    "            self.perturb = perturbation\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Apply the model to some input.\"\"\"\n",
    "        return self.model(x), self.model(self.perturb(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74efcfb-4e9d-4a70-9f55-4c9626ae1cb4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b11a144-870a-4028-95b8-9d6b3e430ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will act like a class\n",
    "def RobustResnet():\n",
    "    \"\"\"\n",
    "    Retrieve a fresh ResNet ready for stability training and fine\n",
    "    tuning.\n",
    "    \"\"\"\n",
    "    bare_resnet = torchvision.models.resnet18(pretrained=True)\n",
    "    for param in bare_resnet.parameters():\n",
    "        param.requires_grad = False\n",
    "    bare_resnet.fc = nn.Linear(512, CLASS_NUMBER, bias=True)\n",
    "    \n",
    "    return StabilityTrainingModel(bare_resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c7da55-0c63-4406-8cdd-0daf01d7dc97",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Training Functions\n",
    "\n",
    "Factoring some common patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e10a4e4-c9ab-457a-818d-d853f38b31a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_epoch(\n",
    "    model,\n",
    "    loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    loss_func,\n",
    "    result,\n",
    "    training=True,\n",
    "    stability_training=False\n",
    "):\n",
    "    loss = torch.Tensor([0])\n",
    "    avg_loss = 0\n",
    "    \n",
    "    if training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    start = time.perf_counter()\n",
    "    with contextlib.ExitStack() as stack: # For validation/training\n",
    "        if not training:\n",
    "            stack.enter_context(torch.no_grad()) # No gradients\n",
    "        for imgs, labels in tqdm_variant(\n",
    "            loader,\n",
    "            desc=f\"{'Training' if training else 'Validation'} Iteration\",\n",
    "            disable=False\n",
    "        ):\n",
    "            if training:\n",
    "                # Prepare optimizer\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Prepare relevant variables\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            out = model(imgs)\n",
    "\n",
    "            # Determine loss\n",
    "            if stability_training:\n",
    "                loss = loss_func(*out, labels)\n",
    "            else:\n",
    "                loss = loss_func(out, labels)\n",
    "\n",
    "            # Store intermediate results - note average over batch\n",
    "            item = loss.item()\n",
    "            avg_loss += item\n",
    "            if training:\n",
    "                result.train_loss_history.append(item)\n",
    "            else:\n",
    "                result.val_loss_history.append(item)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            if stability_training:\n",
    "                acc = stable_classifier_get_accuracy(out, labels)\n",
    "            else:\n",
    "                acc = classifier_get_accuracy(out, labels)\n",
    "            \n",
    "            # Save accuracy\n",
    "            if training:\n",
    "                result.train_accuracy_history.append(acc)\n",
    "            else:\n",
    "                result.val_accuracy_history.append(acc)\n",
    "\n",
    "            if training:\n",
    "                loss.backward() # Get gradients\n",
    "                optimizer.step() # Descend gradients\n",
    "    end = time.perf_counter()\n",
    "    if training:\n",
    "        result.time_training += end - start\n",
    "    else:\n",
    "        result.time_validating += end - start\n",
    "    \n",
    "    return avg_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77288ff1-6e0f-43e2-9780-e3e94312e819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_epoch(\n",
    "    model,\n",
    "    loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    loss_func,\n",
    "    result,\n",
    "    training=True\n",
    "):\n",
    "    loss = torch.Tensor([0])\n",
    "    avg_loss = 0\n",
    "    \n",
    "    if training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    start = time.perf_counter()\n",
    "    with contextlib.ExitStack() as stack: # For validation/training\n",
    "        if not training:\n",
    "            stack.enter_context(torch.no_grad()) # No gradients\n",
    "        for queries, positives, negatives in tqdm_variant(\n",
    "            loader,\n",
    "            desc=f\"{'Training' if training else 'Validation'} Iteration\",\n",
    "            disable=False\n",
    "        ):\n",
    "            if training:\n",
    "                # Prepare optimizer\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Prepare relevant variables\n",
    "            queries = queries.to(device)\n",
    "            positives = positives.to(device)\n",
    "            negatives = negatives.to(device)\n",
    "            out = model(queries, positives, negatives)\n",
    "\n",
    "            # Determine loss\n",
    "            loss = loss_func(*out)\n",
    "\n",
    "            # Store intermediate results - note average over batch\n",
    "            item = loss.item()\n",
    "            avg_loss += item\n",
    "            if training:\n",
    "                result.train_loss_history.append(item)\n",
    "            else:\n",
    "                result.val_loss_history.append(item)\n",
    "\n",
    "            if training:\n",
    "                loss.backward() # Get gradients\n",
    "                optimizer.step() # Descend gradients\n",
    "    end = time.perf_counter()\n",
    "    if training:\n",
    "        result.time_training += end - start\n",
    "    else:\n",
    "        result.time_validating += end - start\n",
    "    \n",
    "    return avg_loss / len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce64b9b-dca7-4fd0-b015-65011d31d691",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Miscellaneous Helpers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54635cd8-2907-402c-966b-2cd603a47396",
   "metadata": {},
   "source": [
    "Some constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e03d5a6b-2e35-468c-80c1-a84c55c0de08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight of the stability objective\n",
    "ALPHA = 0.01\n",
    "\n",
    "CLASS_NUMBER = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6658653-1a46-45dc-8811-6864122d7697",
   "metadata": {},
   "source": [
    "Loss function things for stability trained classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ad874a9-2422-4730-aa06-4280b1dc46df",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_objective = nn.CrossEntropyLoss()\n",
    "classifier_stability_loss = nn.CrossEntropyLoss()\n",
    "distort_envelope = nn.Softmax(dim=1) # Converts to probabilities\n",
    "\n",
    "def full_loss(clean_output, distort_output, label):\n",
    "    \"\"\"Note that this will apply softmax to distort_output.\"\"\"\n",
    "    return (\n",
    "        classifier_objective(clean_output, label)\n",
    "        + ALPHA * classifier_stability_loss(clean_output, distort_envelope(distort_output))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4ed5d1-87f5-4f24-a08f-955e19bb425b",
   "metadata": {},
   "source": [
    "ResNet is made to be used with data transformed as so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74886e62-5e7a-42b2-84fd-3f43dccac3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_transform = T.Compose([\n",
    "    T.Resize((256,256)),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798010ca-f022-41e7-9b9a-7c4ed0beefcc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Training\n",
    "\n",
    "Now, let's do some training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9a84a5-bf45-4618-8052-a7ed3c1ae52b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed259b-09fc-454e-862a-99b90a81a645",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Baseline\n",
    "\n",
    "Let's train a baseline ResNet. First, prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d71b6f73-230b-45b2-8cf7-57edd10fd793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dataset_train = datasets.ImageFolder(\n",
    "    os.path.join(data_dir, \"train\"),\n",
    "    transform=resnet_transform\n",
    ")\n",
    "class_dataset_val = datasets.ImageFolder(\n",
    "    os.path.join(data_dir, \"val\"),\n",
    "    transform=resnet_transform\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    class_dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    class_dataset_val,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634133e2-ffbb-40ea-a6cb-83dc522bcac7",
   "metadata": {},
   "source": [
    "Next, prepare the model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3ea6b25-5db6-451e-bbe6-93047a5d1f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default variation. Note pretrained\n",
    "resnet = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all layers\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Replace last, note now unfrozen and we'll fine tune\n",
    "resnet.fc = nn.Linear(512, 200, bias=True)\n",
    "\n",
    "# Send to GPU, if possible\n",
    "resnet = resnet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03e3211-f51b-4b6a-9324-c4fb48c15b4e",
   "metadata": {},
   "source": [
    "Learning controllers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "166c6ebc-e8b3-4ee7-9a27-a237eabca507",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(resnet.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    threshold=0.1\n",
    ")\n",
    "# May try OneCycleLR, annealers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d9e6a6-850c-4996-8857-4bc82de6ba29",
   "metadata": {},
   "source": [
    "The loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "731a21ec-4940-4ce9-954f-80ab7bf8a1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_loss = classifier_objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293e28ce-565a-49ad-8925-04eef82fa8b2",
   "metadata": {},
   "source": [
    "Keep the result in here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c35154fa-52ad-4e12-84b5-32bae925ab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_result = TrainResult(resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ae5aa8-6049-48fe-9122-2894bbb2435f",
   "metadata": {},
   "source": [
    "The actual training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1771747e-f693-4e77-9cd2-1e61881aea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for some number of epochs\n",
    "EPOCH_COUNT = 1#20\n",
    "SAVE_PERIOD = 3\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "best_count = 0\n",
    "for epoch in tqdm_variant(\n",
    "    range(EPOCH_COUNT),\n",
    "    desc=f\"Epoch\",\n",
    "    unit=\"epoch\",\n",
    "    disable=False\n",
    "):\n",
    "    # Debug learning rate\n",
    "    print(f\"Current Learning Rate: {optimizer.param_groups[0]['lr']}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = classifier_epoch(\n",
    "        resnet,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        device,\n",
    "        resnet_loss,\n",
    "        resnet_result\n",
    "    )\n",
    "\n",
    "    # Validation\n",
    "    val_loss = classifier_epoch(\n",
    "        resnet,\n",
    "        val_loader,\n",
    "        device,\n",
    "        resnet_loss,\n",
    "        resnet_result,\n",
    "        training=False\n",
    "    )\n",
    "    \n",
    "    # Update scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save a copy just in case\n",
    "    if val_loss < best_loss:\n",
    "        torch.save(\n",
    "            resnet, \n",
    "            os.path.join(\n",
    "                os.path.abspath(\"\"),\n",
    "                f\"best_resnet_classifier_{best_count}_{int(time.time())}.pt\"\n",
    "            )\n",
    "        )\n",
    "        best_count += 1\n",
    "        best_loss = val_loss\n",
    "        \n",
    "    if epoch % SAVE_PERIOD == SAVE_PERIOD - 1:\n",
    "        torch.save(\n",
    "            resnet,\n",
    "            os.path.join(\n",
    "                os.path.abspath(\"\"),\n",
    "                f\"resnet_classifier_{epoch}_{int(time.time())}.pt\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Print the last loss calculated and the epoch\n",
    "    print(f\"\\nEpoch {epoch}: Training Loss: {train_loss}, \" \\\n",
    "          f\"Validation Loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7bf9a7-0dab-414d-bc04-0f2c9a82daf0",
   "metadata": {},
   "source": [
    "Let's view the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96da7756-7952-4b17-bfe2-2ab465476495",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "resnet_result.full_analysis(axs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3519076e-65ad-44e5-88fc-eed36f7886fe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Stability Trained\n",
    "\n",
    "Let's stability train ResNet. We can use the same datasets and loaders. Let's prepare the model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1ef0bc01-cf29-4507-8445-4a2256e5ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_resnet = RobustResnet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38a8a35-ef2d-4d6e-a6a0-d048f45efd33",
   "metadata": {},
   "source": [
    "First, a quick sanity check of gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0b6cc7ff-f96b-40c5-9619-17643ae6fa50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 3.0.0 (20220226.1711)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"299pt\" height=\"281pt\"\n",
       " viewBox=\"0.00 0.00 299.00 281.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 277)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-277 295,-277 295,4 -4,4\"/>\n",
       "<!-- 1905763096592 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>1905763096592</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"255,-31 184,-31 184,0 255,0 255,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"219.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (1, 200)</text>\n",
       "</g>\n",
       "<!-- 1905756694368 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1905756694368</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"270,-86 169,-86 169,-67 270,-67 270,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"219.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 1905756694368&#45;&gt;1905763096592 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>1905756694368&#45;&gt;1905763096592</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M219.5,-66.79C219.5,-60.07 219.5,-50.4 219.5,-41.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"223,-41.19 219.5,-31.19 216,-41.19 223,-41.19\"/>\n",
       "</g>\n",
       "<!-- 1905756694704 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1905756694704</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"291,-141 190,-141 190,-122 291,-122 291,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"240.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1905756694704&#45;&gt;1905756694368 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>1905756694704&#45;&gt;1905756694368</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M237.03,-121.75C234.22,-114.65 230.16,-104.4 226.65,-95.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"229.84,-94.1 222.9,-86.09 223.33,-96.68 229.84,-94.1\"/>\n",
       "</g>\n",
       "<!-- 1905756693264 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>1905756693264</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"130,-86 29,-86 29,-67 130,-67 130,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"79.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 1905756694704&#45;&gt;1905756693264 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>1905756694704&#45;&gt;1905756693264</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M214.63,-121.98C187.72,-113.13 145.38,-99.19 115.27,-89.28\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"116.16,-85.88 105.57,-86.08 113.97,-92.53 116.16,-85.88\"/>\n",
       "</g>\n",
       "<!-- 1905756582432 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>1905756582432</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"288,-207 193,-207 193,-177 288,-177 288,-207\"/>\n",
       "<text text-anchor=\"middle\" x=\"240.5\" y=\"-195\" font-family=\"monospace\" font-size=\"10.00\">model.fc.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"240.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\"> (200)</text>\n",
       "</g>\n",
       "<!-- 1905756582432&#45;&gt;1905756694704 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1905756582432&#45;&gt;1905756694704</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M240.5,-176.84C240.5,-169.21 240.5,-159.7 240.5,-151.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"244,-151.27 240.5,-141.27 237,-151.27 244,-151.27\"/>\n",
       "</g>\n",
       "<!-- 1905756695088 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>1905756695088</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"172,-141 95,-141 95,-122 172,-122 172,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"133.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 1905756695088&#45;&gt;1905756694368 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>1905756695088&#45;&gt;1905756694368</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M147.32,-121.98C160.65,-113.77 181.08,-101.18 196.81,-91.49\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"198.75,-94.4 205.43,-86.17 195.08,-88.44 198.75,-94.4\"/>\n",
       "</g>\n",
       "<!-- 1905756696432 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>1905756696432</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"132,-201.5 31,-201.5 31,-182.5 132,-182.5 132,-201.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"81.5\" y=\"-189.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1905756696432&#45;&gt;1905756695088 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>1905756696432&#45;&gt;1905756695088</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M89.17,-182.37C97.01,-173.55 109.36,-159.66 119.04,-148.77\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"121.77,-150.97 125.8,-141.17 116.54,-146.32 121.77,-150.97\"/>\n",
       "</g>\n",
       "<!-- 1905756695616 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>1905756695616</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"77,-141 0,-141 0,-122 77,-122 77,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"38.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 1905756696432&#45;&gt;1905756695616 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>1905756696432&#45;&gt;1905756695616</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M75.16,-182.37C68.8,-173.73 58.86,-160.2 50.93,-149.42\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"53.61,-147.15 44.87,-141.17 47.97,-151.3 53.61,-147.15\"/>\n",
       "</g>\n",
       "<!-- 1905755740016 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>1905755740016</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"135,-273 28,-273 28,-243 135,-243 135,-273\"/>\n",
       "<text text-anchor=\"middle\" x=\"81.5\" y=\"-261\" font-family=\"monospace\" font-size=\"10.00\">model.fc.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"81.5\" y=\"-250\" font-family=\"monospace\" font-size=\"10.00\"> (200, 512)</text>\n",
       "</g>\n",
       "<!-- 1905755740016&#45;&gt;1905756696432 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>1905755740016&#45;&gt;1905756696432</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M81.5,-242.8C81.5,-233.7 81.5,-221.79 81.5,-211.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"85,-211.84 81.5,-201.84 78,-211.84 85,-211.84\"/>\n",
       "</g>\n",
       "<!-- 1905763097712 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>1905763097712</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"115,-31 44,-31 44,0 115,0 115,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"79.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (1, 200)</text>\n",
       "</g>\n",
       "<!-- 1905756693264&#45;&gt;1905763097712 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>1905756693264&#45;&gt;1905763097712</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M79.5,-66.79C79.5,-60.07 79.5,-50.4 79.5,-41.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"83,-41.19 79.5,-31.19 76,-41.19 83,-41.19\"/>\n",
       "</g>\n",
       "<!-- 1905756695616&#45;&gt;1905756693264 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>1905756695616&#45;&gt;1905756693264</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M45.27,-121.75C51.06,-114.26 59.56,-103.28 66.64,-94.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"69.51,-96.14 72.86,-86.09 63.97,-91.86 69.51,-96.14\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1bbb7f36ac0>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_sanity_display(\n",
    "    stable_resnet,\n",
    "    [next(iter(train_loader))[0][0].reshape(1, 3, 224, 224)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031690c3-b58f-4924-ab62-16a5df8f3b9c",
   "metadata": {},
   "source": [
    "Next sanity check, how are the weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bd62f0ef-fc1e-4c6e-8154-a62137cb8bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.0073, -0.0071, -0.0009,  ...,  0.0216,  0.0329, -0.0283],\n",
      "        [-0.0129, -0.0424,  0.0151,  ...,  0.0160,  0.0379, -0.0258],\n",
      "        [ 0.0290,  0.0259, -0.0220,  ...,  0.0002, -0.0416,  0.0307],\n",
      "        ...,\n",
      "        [ 0.0433, -0.0088, -0.0040,  ...,  0.0174,  0.0291, -0.0318],\n",
      "        [ 0.0203, -0.0184, -0.0100,  ...,  0.0415, -0.0289, -0.0232],\n",
      "        [-0.0161,  0.0177,  0.0332,  ...,  0.0050, -0.0275,  0.0085]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 1.8264e-02,  8.2651e-03, -1.8287e-02, -5.1264e-03,  2.3474e-02,\n",
      "         3.6429e-02, -2.5332e-02,  3.1460e-02, -9.8708e-03, -3.0398e-02,\n",
      "        -3.8438e-02, -1.3270e-02,  9.8759e-04, -4.4047e-02,  2.5429e-02,\n",
      "         1.5400e-02,  8.2721e-03, -1.7918e-03,  6.0061e-03,  2.8976e-02,\n",
      "         8.5340e-03,  2.2457e-02, -1.9287e-02, -2.0829e-02,  3.4783e-02,\n",
      "        -2.5888e-02, -1.2726e-02, -3.0649e-02,  3.8350e-02, -3.2814e-02,\n",
      "         1.2518e-02,  2.4499e-02,  2.5248e-02, -1.3182e-02, -1.8951e-02,\n",
      "        -2.5125e-02,  3.0337e-02, -1.8168e-02,  2.7848e-02, -3.5343e-02,\n",
      "         2.5557e-02,  9.0833e-03,  3.1933e-02,  8.4515e-03, -2.9782e-02,\n",
      "         4.6637e-04,  1.6648e-02,  3.6029e-02, -5.7736e-04, -3.2324e-02,\n",
      "         1.7425e-02,  2.4118e-02,  2.0374e-04, -1.7792e-02,  2.6119e-02,\n",
      "         7.6434e-03,  3.1694e-02, -8.2811e-03, -1.8600e-02,  1.3295e-02,\n",
      "         3.3866e-02, -3.6468e-02, -3.3668e-02, -1.2283e-02,  2.8081e-02,\n",
      "        -7.1637e-05,  2.5853e-03, -2.7307e-03,  2.7685e-02,  2.2582e-02,\n",
      "         2.2255e-02, -1.5721e-02,  1.9452e-02, -3.8851e-02, -3.0974e-02,\n",
      "         8.4646e-03,  4.0635e-02, -1.9948e-02,  2.5081e-02, -1.0115e-02,\n",
      "        -4.2984e-02, -1.0389e-02, -1.5659e-02, -2.6995e-02, -4.3353e-02,\n",
      "        -2.1492e-02, -1.4319e-02,  1.1528e-02,  3.3912e-02, -1.9015e-02,\n",
      "        -4.1259e-02,  2.0605e-02, -2.0210e-04,  9.2464e-04, -3.9788e-02,\n",
      "         6.4819e-03,  8.0231e-03,  1.1676e-03, -4.1002e-02, -1.8272e-02,\n",
      "        -4.0126e-02, -3.5444e-02,  5.7033e-03,  2.6816e-02,  1.1367e-02,\n",
      "         4.0502e-02,  3.6346e-02, -3.7162e-02, -4.2701e-02,  4.2548e-02,\n",
      "         3.9660e-02, -3.8565e-02, -7.5670e-03,  1.5610e-02, -3.7134e-03,\n",
      "         3.6108e-02,  4.2515e-03,  3.5431e-02, -3.0926e-02, -1.5563e-02,\n",
      "        -1.4019e-03, -3.9361e-02, -3.1111e-02,  3.2839e-02, -8.5036e-03,\n",
      "        -1.3573e-02, -1.9182e-02,  2.3021e-02, -4.4122e-02, -1.7821e-02,\n",
      "        -2.1573e-02,  3.9856e-02, -1.5094e-02,  2.1381e-02, -1.0357e-02,\n",
      "         7.2276e-04,  4.1654e-02, -4.0949e-02,  1.3562e-03,  4.1846e-02,\n",
      "         1.5767e-02, -1.3102e-02, -6.7436e-03,  1.4848e-02, -1.2490e-02,\n",
      "         3.2020e-02,  2.9031e-02,  6.4261e-03, -1.6270e-02, -1.0201e-03,\n",
      "         1.7042e-02, -4.2427e-02,  8.7204e-03, -3.9955e-02, -3.9531e-02,\n",
      "         3.9461e-02, -1.8206e-02,  1.8873e-02,  9.0815e-03,  3.7462e-02,\n",
      "        -1.3486e-02, -1.6315e-02,  3.9678e-02,  3.0658e-02, -1.9430e-02,\n",
      "         3.1016e-02, -3.5792e-02,  2.6776e-02,  1.3346e-02,  2.8839e-03,\n",
      "        -5.1058e-03,  3.6036e-02, -4.1753e-02, -2.0651e-02,  2.1401e-02,\n",
      "        -2.9639e-02, -4.3932e-02,  9.8751e-03,  3.6776e-02,  5.7029e-03,\n",
      "        -2.3731e-02,  2.5239e-02, -4.4126e-02,  2.8335e-04, -4.0261e-02,\n",
      "        -3.4357e-02, -3.6339e-02, -3.8356e-02,  2.9664e-02, -3.3434e-02,\n",
      "         1.9505e-02, -4.0687e-02, -6.6193e-03, -1.5648e-02, -3.6014e-02,\n",
      "         3.3670e-02,  3.9076e-02, -4.1744e-02,  1.9638e-02, -7.8499e-03],\n",
      "       device='cuda:0', requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(stable_resnet.model.fc.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1c4e09-d433-479a-b96f-a02feb088b9a",
   "metadata": {},
   "source": [
    "Learning controllers, note that learning rate starts slower now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "906407a4-262c-40c0-a7a2-68930baccfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_optimizer = optim.SGD(stable_resnet.parameters(), lr=0.005)\n",
    "s_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    s_optimizer,\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    threshold=0.1\n",
    ")\n",
    "# May try OneCycleLR, annealers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cc14d8-25cc-41fa-887d-e2aeb3919c09",
   "metadata": {},
   "source": [
    "The loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "768a0af9-1bcd-4262-bb43-161ea9f952a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_resnet_loss = full_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc86eb8-7046-40b2-884f-897440c92844",
   "metadata": {},
   "source": [
    "Keep the result in here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "84a0cbf8-1d78-48b2-83c7-a135aba2899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_resnet_result = TrainResult(stable_resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bf14ea-5067-49d0-a28a-2928f0adb4e8",
   "metadata": {},
   "source": [
    "The actual training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "15d18fc2-0d71-444f-9d1d-9b552213383d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93dcb16f4ba34233b3775d82407a8646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/20 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Learning Rate: 0.005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e8da37328b4b4e882b805320c4c314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Iteration:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5724e742e424901a3cd0e4d7d14ad1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0: Training Loss: 4.727948836767742, Validation Loss: 4.084477017639549\n",
      "Current Learning Rate: 0.005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfae53af77a244a8b5f26d132bc3b6d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Iteration:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c440356867a4fb09c217def280f13e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: Training Loss: 3.674960175311039, Validation Loss: 3.2731017041358217\n",
      "Current Learning Rate: 0.005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f3792b1bd3439faafb462cac580471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Iteration:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e35d144f416413e91a3ef7cf6f60d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: Training Loss: 3.079790781189521, Validation Loss: 2.844098055438631\n",
      "Current Learning Rate: 0.005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f59046a692549ba86d85758af3e58de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Iteration:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8d3539376d44a99e256fd2f1b9ebf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: Training Loss: 2.735764454819038, Validation Loss: 2.579060474019142\n",
      "Current Learning Rate: 0.005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30bdc7ee3f145c884627048b921905f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Iteration:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b0c27045ed4dfcb02844de1b69c8f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: Training Loss: 2.5209649991165426, Validation Loss: 2.4024654391464915\n",
      "Current Learning Rate: 0.005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49970733becc43a3ace5205e6d8cb4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Iteration:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473d2754ef484e1e9c0490adc1660d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: Training Loss: 2.376510003630503, Validation Loss: 2.3007565706398836\n",
      "Current Learning Rate: 0.005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d7dfa1e5304228b4306f670524f56d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Iteration:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e2b19dcc5a40699a5ae185e584b330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: Training Loss: 2.2721807902544184, Validation Loss: 2.2295108946265687\n",
      "Current Learning Rate: 0.005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59dfb5bd8bee42b693acbb4999d041ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Iteration:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d142d9ae0084451803bb92387972bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: Training Loss: 2.192590256875246, Validation Loss: 2.1606985266041603\n",
      "Current Learning Rate: 0.005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6162c8f9ee5046f7ae6d65d1123614fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Iteration:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ff4fec11a54aebb0c3fd25b49916a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: Training Loss: 2.130990964208592, Validation Loss: 2.1133614839262265\n",
      "Current Learning Rate: 0.005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "193fd2ccd4514c028d37723330d547dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Iteration:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32286cc38274d7a9c0c03b8896784fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: Training Loss: 2.0818074992735722, Validation Loss: 2.0866039023277865\n",
      "Current Learning Rate: 0.005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa46c2cfecaa4fc38d09ed13d1302d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Iteration:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23eea3243b7d40169ef1652da3e85674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: Training Loss: 2.0409617253930157, Validation Loss: 2.048759793020358\n",
      "Current Learning Rate: 0.0025\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "132f5f4af09542b499c0c9e68b0a364b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Iteration:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7b44faa655449386e19a9f43364740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11: Training Loss: 2.008505058303828, Validation Loss: 2.0371640267645477\n",
      "Current Learning Rate: 0.0025\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb85beb88bfa4dcfb74910419427a607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Iteration:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0160ec3da43493f9eaa3024859c8ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12: Training Loss: 1.992997704754254, Validation Loss: 2.0173599731390643\n",
      "Current Learning Rate: 0.0025\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13cc5a4659a642ddafb8529c9f41aec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Iteration:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1d7ba8868b472ea897a590b7174bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13: Training Loss: 1.9773476194740485, Validation Loss: 2.007489614046303\n",
      "Current Learning Rate: 0.00125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15796339ab44da3aee551acb47edd4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Iteration:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efff755fd87a4b4d87e263413a0fda26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: Training Loss: 1.9642766243513012, Validation Loss: 2.0208338631945812\n",
      "Current Learning Rate: 0.00125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39908953154a454cb7ec729f0be57ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Iteration:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed22809b04b42c3bd6d6817f0a6627f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15: Training Loss: 1.9556046149247133, Validation Loss: 1.9952780638530756\n",
      "Current Learning Rate: 0.00125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d6b0bd7d2384f0197066fa07bf2b02a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Iteration:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0e49a44a0c40e5b61aafaf6da68694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16: Training Loss: 1.9502355649695515, Validation Loss: 2.0010284640986447\n",
      "Current Learning Rate: 0.000625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d689a8dbb849829fd29433c9756ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Iteration:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff53607dc4645d0836a0fd61967dfdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17: Training Loss: 1.9436827919380983, Validation Loss: 1.9855448728913714\n",
      "Current Learning Rate: 0.000625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22f613cd9524f04a57fa787c1513177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Iteration:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ff1fe15a9d40259afc86fcb565193d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18: Training Loss: 1.9389324397973655, Validation Loss: 1.9762624524960852\n",
      "Current Learning Rate: 0.000625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "747e0bb5b59b4ed7b4ccfbd623791551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Iteration:   0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eddec039e7949e5aa60ffbb4599016b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19: Training Loss: 1.9368133958302776, Validation Loss: 1.975199190674314\n"
     ]
    }
   ],
   "source": [
    "# Train for some number of epochs\n",
    "S_EPOCH_COUNT = 20\n",
    "S_SAVE_PERIOD = 3\n",
    "\n",
    "s_best_loss = float(\"inf\")\n",
    "s_best_count = 0\n",
    "for epoch in tqdm_variant(\n",
    "    range(S_EPOCH_COUNT),\n",
    "    desc=f\"Epoch\",\n",
    "    unit=\"epoch\",\n",
    "    disable=False\n",
    "):\n",
    "    # Debug learning rate\n",
    "    print(f\"Current Learning Rate: {s_optimizer.param_groups[0]['lr']}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = classifier_epoch(\n",
    "        stable_resnet,\n",
    "        train_loader,\n",
    "        s_optimizer,\n",
    "        device,\n",
    "        s_resnet_loss,\n",
    "        s_resnet_result,\n",
    "        stability_training=True\n",
    "    )\n",
    "\n",
    "    # Validation\n",
    "    val_loss = classifier_epoch(\n",
    "        stable_resnet,\n",
    "        val_loader,\n",
    "        s_optimizer,\n",
    "        device,\n",
    "        s_resnet_loss,\n",
    "        s_resnet_result,\n",
    "        training=False,\n",
    "        stability_training=True\n",
    "    )\n",
    "    \n",
    "    # Update scheduling\n",
    "    s_scheduler.step(val_loss)\n",
    "    \n",
    "    # Save a copy just in case\n",
    "    if val_loss < s_best_loss:\n",
    "        torch.save(\n",
    "            stable_resnet, \n",
    "            os.path.join(\n",
    "                os.path.abspath(\"\"),\n",
    "                f\"best_stable_resnet_classifier_{s_best_count}\" \\\n",
    "                f\"_{int(time.time()) % 100000:06}.pt\"\n",
    "            )\n",
    "        )\n",
    "        s_best_count += 1\n",
    "        s_best_loss = val_loss\n",
    "        \n",
    "    if epoch % S_SAVE_PERIOD == S_SAVE_PERIOD - 1:\n",
    "        torch.save(\n",
    "            stable_resnet,\n",
    "            os.path.join(\n",
    "                os.path.abspath(\"\"),\n",
    "                f\"stable_resnet_classifier_{epoch}_\" \\\n",
    "                f\"{int(time.time()) % 100000:06}.pt\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Print the average loss calculated and the epoch\n",
    "    print(f\"\\nEpoch {epoch}: Training Loss: {train_loss}, \" \\\n",
    "          f\"Validation Loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edad7c0-8c6f-4794-ad27-4db8ea3e31f4",
   "metadata": {},
   "source": [
    "Let's view the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8f888d2d-d945-4fc8-8ef4-77f603067abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent 5141 seconds training.\n",
      "Spent 501 seconds evaluating.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "s_resnet_result.full_analysis(axs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418f33c9-33ee-420b-b3b5-c29060e2d864",
   "metadata": {},
   "source": [
    "Next, let's save it with a special name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ae1c0828-7649-42a0-98f3-87639f238a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    stable_resnet, \n",
    "    os.path.join(\n",
    "        os.path.abspath(\"\"),\n",
    "        \"stability_resnet.pt\"\n",
    "    )\n",
    ")\n",
    "s_resnet_result.save(_def_save_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d009cd7c-cec2-4c08-8b22-f3c0c4fa59b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Triplet Ranking\n",
    "\n",
    "Next, let's train some triplet ranking models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b366d7f3-6e61-4dc4-9a49-38a60ee85b6a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Baseline\n",
    "\n",
    "Prepare data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b7ce7f81-c188-42e7-9082-117f2556a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(data_dir, \"train\")\n",
    "val_path = os.path.join(data_dir, \"val\")\n",
    "trd = TripletRankingDataset(train_path, 2, 2, transform=resnet_transform)\n",
    "trd_val = TripletRankingDataset(\n",
    "    val_path,\n",
    "    1,\n",
    "    1,\n",
    "    transform=resnet_transform,\n",
    "    training=False\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "tr_train_loader = torch.utils.data.DataLoader(trd, batch_size=batch_size,\n",
    "                                     num_workers=0, shuffle=True)\n",
    "tr_val_loader = torch.utils.data.DataLoader(trd_val, batch_size=batch_size,\n",
    "                                     num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331d938f-53c6-4f61-9e2b-d0a37b1fe9b4",
   "metadata": {},
   "source": [
    "Prepare new model that maps into a 64 dimensional encoding space. Note that this is less than the 200 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "df26a010-38a2-42c9-be99-c80c0b343749",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_encoder = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "for param in resnet_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "resnet_encoder.fc = nn.Linear(512, 64, bias=True) # 64-Dimensional Encoding\n",
    "\n",
    "resnet_triplet_ranking = TripletRanker(resnet_encoder).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69dc77d-6ad0-4976-985d-fbf1b46a8516",
   "metadata": {},
   "source": [
    "Training controllers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5a778ad3-0312-4f12-8a58-48cb8847caa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtr_optimizer = optim.SGD(resnet_triplet_ranking.parameters(), lr=0.01)\n",
    "rtr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    rtr_optimizer,\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    threshold=0.1\n",
    ")\n",
    "# May try OneCycleLR, annealers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2461a585-7d7d-45fa-9324-5556ccf70095",
   "metadata": {},
   "source": [
    "Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cc8db298-59a6-4925-896a-4ab0753ff472",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtr_result = TrainResult(resnet_triplet_ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6d7f05-c0e8-44c5-a18e-efcda7283164",
   "metadata": {},
   "source": [
    "Triplet loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "18086897-8081-4a74-92d3-d0eb5fd0c140",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtr_loss = nn.TripletMarginLoss(1.0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954ad413-8966-4cfc-8530-8061c4f12def",
   "metadata": {},
   "source": [
    "Training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d095a291-5ed8-46e5-b81b-42bd2079315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for some number of epochs\n",
    "RTR_EPOCH_COUNT = 20\n",
    "RTR_SAVE_PERIOD = 3\n",
    "\n",
    "loss = torch.Tensor([0])\n",
    "rtr_best_loss = float(\"inf\")\n",
    "rtr_best_count = 0\n",
    "for epoch in tqdm_variant(\n",
    "    range(RTR_EPOCH_COUNT),\n",
    "    desc=f\"Epoch\",\n",
    "    unit=\"epoch\",\n",
    "    disable=False\n",
    "):\n",
    "    # Debug learning rate\n",
    "    print(f\"Current Learning Rate: {rtr_optimizer.param_groups[0]['lr']}\")\n",
    "    \n",
    "    train_loss = triplet_epoch(\n",
    "        resnet_triplet_ranking,\n",
    "        tr_train_loader,\n",
    "        rtr_optimizer,\n",
    "        device,\n",
    "        rtr_loss,\n",
    "        rtr_result\n",
    "    )\n",
    "\n",
    "    val_loss = triplet_epoch(\n",
    "        resnet_triplet_ranking,\n",
    "        tr_val_loader,\n",
    "        rtr_optimizer,\n",
    "        device,\n",
    "        rtr_loss,\n",
    "        rtr_result,\n",
    "        training=False\n",
    "    )\n",
    "    \n",
    "    # Save a copy for safety\n",
    "    if val_loss < rtr_best_loss:\n",
    "        torch.save(\n",
    "            resnet_triplet_ranking, \n",
    "            os.path.join(\n",
    "                os.path.abspath(\"\"),\n",
    "                f\"best_rtr_{best_count}_{int(time.time()) % 1000000:06}.pt\"\n",
    "            )\n",
    "        )\n",
    "        rtr_best_count += 1\n",
    "        rtr_best_loss = val_loss\n",
    "        \n",
    "    if epoch % RTR_SAVE_PERIOD == RTR_SAVE_PERIOD - 1:\n",
    "        torch.save(\n",
    "            resnet_triplet_ranking,\n",
    "            os.path.join(\n",
    "                os.path.abspath(\"\"),\n",
    "                f\"rtr_{epoch}_{int(time.time()) % 1000000:06}.pt\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Print the last loss calculated and the epoch\n",
    "    print(f\"\\nEpoch {epoch}: Training Loss: {train_loss}, \" \\\n",
    "          f\"Validation Loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5941e0f-b182-4daf-b917-33d7283c414c",
   "metadata": {},
   "source": [
    "Let's double check gradient calculations went well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7b5313-cd95-4b07-a9c9-7c550874fc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_sanity_display(resnet_triplet_ranking, next(iter(tr_train_loader))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b139ab77-5f5b-406f-afcf-bd716ed4d2cf",
   "metadata": {},
   "source": [
    "#### Stability Trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c16049e-8833-4dc6-8c23-ce9e6f8c5af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4952048-8638-4874-b336-40d4a280ec07",
   "metadata": {},
   "source": [
    "## Experiments and Plot Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6a866a-07ab-4d1c-96e0-8254cb686f1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Classifier vs. JPEG\n",
    "\n",
    "We'll see how precision declines with more JPEG. First, let's load the classifiers from save files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0a6ef26d-e48d-4976-bd92-fbec7b31f604",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_stabilized_classifier = torch.load(\n",
    "    os.path.join(\n",
    "        os.path.abspath(\"\"),\n",
    "        \"stability_resnet.pt\"\n",
    "    ),\n",
    "    device\n",
    ")\n",
    "\n",
    "loaded_baseline_classifier = torch.load(\n",
    "    os.path.join(\n",
    "        os.path.abspath(\"\"),\n",
    "        \"resnet.pt\"\n",
    "    ),\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f415b3be-13e7-47f8-9d40-c75f0a1a71a9",
   "metadata": {},
   "source": [
    "Sanity checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "049ef014-c1f2-4704-a3ee-6b4a233eea4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=200, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(loaded_baseline_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "83f91dd8-12a2-419b-8e1c-dab699aab0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StabilityTrainingModel(\n",
      "  (model): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=512, out_features=200, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(loaded_stabilized_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e5cf7333-b9e0-4a26-ae7d-2af8537d4205",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11328/2035251330.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m gradient_sanity_display(\n\u001b[0;32m      2\u001b[0m     \u001b[0mloaded_baseline_classifier\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;33m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "gradient_sanity_display(\n",
    "    loaded_baseline_classifier,\n",
    "    [next(iter(train_loader))[0][0].reshape(1, 3, 224, 224)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "53cdca1f-aeca-4ac0-9253-6cd880683ac8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11328/1923717546.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m gradient_sanity_display(\n\u001b[0;32m      2\u001b[0m     \u001b[0mloaded_stabilized_classifier\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;33m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "gradient_sanity_display(\n",
    "    loaded_stabilized_classifier,\n",
    "    [next(iter(train_loader))[0][0].reshape(1, 3, 224, 224)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a30d1d-db82-4d33-a9b6-a7b16b68b96b",
   "metadata": {},
   "source": [
    "Looking good, so we're ready. We'll increase the JPEG levels as we experiment. Note that we can reuse some code for this. The data collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f645baad-4bc0-4b95-8a35-9f8af1b0a826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d26e98485b2749c7ab080399ede2ba85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Experiment:   0%|          | 0/10 [00:00<?, ?experiment/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f496dae28c4d1c8b712ffd7c999974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac99605b338b4eddbbcd4612c560740e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299e62abe277402f8e3ee2a89fb9130f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a060245fee2c46b896a7bda99ab1a944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf2c186eb3d40419ce3f8ce01ad26ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b02812a3d54e85bb0d19cfb1c74fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc093c04be048909f5156b32f4d1379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f5cb10c6d746408126dc7e9b3343c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44c9bbca67e4960a715422530ce51aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c3c1703e45240e085a741313b4bffef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db646123474437f88acc8401c727012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4ba55f8032485e907c35c16c9a5943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d0c70496d7a4c0896e39eb929110a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ad608d771840a692adc4e8202f4688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c580fd7172b64263a7291ad9966efe54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7db10c47794ed792074cc0d0f2b87b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17140ddfac7644588699fba4d3c238da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544fbcab82a444f79ce98c4019101a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f7ef32a6d84c6c9b41b057bcd9f3fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e6f4aa46d8456f8fce71975f9f7c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Iteration:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE = 512\n",
    "\n",
    "qualities = list(range(100, 0, -10))\n",
    "baseline_precision = [ ]\n",
    "stabilized_precision = [ ]\n",
    "\n",
    "for quality in tqdm_variant(\n",
    "    qualities,\n",
    "    desc=f\"Experiment\",\n",
    "    unit=\"experiment\"\n",
    "):\n",
    "    #\n",
    "    # Prepare the data\n",
    "    #\n",
    "    apply_jpeg = JPEGTransform(quality)\n",
    "    \n",
    "    this_dataset = datasets.ImageFolder(\n",
    "        os.path.join(data_dir, \"val\"),\n",
    "        transform=T.Compose([apply_jpeg, resnet_transform])\n",
    "    )\n",
    "    this_loader = torch.utils.data.DataLoader(\n",
    "        this_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    tmp_result = TrainResult(None)\n",
    "\n",
    "    #\n",
    "    # Experiments\n",
    "    #\n",
    "    \n",
    "    # Perform stabilized experiment\n",
    "    _ = classifier_epoch(\n",
    "            loaded_baseline_classifier,\n",
    "            this_loader,\n",
    "            None,\n",
    "            device,\n",
    "            resnet_loss,\n",
    "            tmp_result,\n",
    "            training=False\n",
    "        )\n",
    "    _ = classifier_epoch(\n",
    "            loaded_stabilized_classifier,\n",
    "            this_loader,\n",
    "            None,\n",
    "            device,\n",
    "            s_resnet_loss,\n",
    "            tmp_result,\n",
    "            training=False,\n",
    "            stability_training=True\n",
    "        )\n",
    "    \n",
    "    #\n",
    "    # Extract data\n",
    "    #\n",
    "    trial_size = len(tmp_result.val_accuracy_history) // 2\n",
    "    inv_img_count = 1.0 / (len(this_dataset) // BATCH_SIZE)\n",
    "    # Calculate baseline\n",
    "    baseline_precision.append(\n",
    "        sum(tmp_result.val_accuracy_history[:trial_size - 1]) * inv_img_count\n",
    "    )\n",
    "    stabilized_precision.append(\n",
    "        sum(tmp_result.val_accuracy_history[trial_size:-1]) * inv_img_count\n",
    "    )\n",
    "\n",
    "    # No chances with memory management\n",
    "    del tmp_result, this_loader, this_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "496f8c99-6a08-458a-b372-8c432e29c681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(qualities, baseline_precision, label=\"Baseline\")\n",
    "plt.plot(qualities, stabilized_precision, label=r\"$\\alpha=0.01$\")\n",
    "plt.xticks(qualities[2:-1:2], [f\"{quality}.00%\" for quality in qualities[2:-1:2]])\n",
    "plt.xlim(100, 0)\n",
    "plt.xlabel(\"JPEG quality\", fontweight=\"bold\", loc=\"left\")\n",
    "plt.ylabel(\"Precision@top-1\", loc=\"top\")\n",
    "plt.gca().yaxis.tick_right()\n",
    "plt.gca().yaxis.set_label_position(\"right\")\n",
    "plt.gca().tick_params(axis=\"x\", direction=\"in\", pad=-15)\n",
    "plt.gca().tick_params(axis=\"y\", direction=\"in\", pad=-22)\n",
    "plt.legend(loc=\"upper center\")\n",
    "plt.grid()\n",
    "fig = plt.gcf()\n",
    "fig.savefig(\n",
    "    os.path.join(\n",
    "        os.path.abspath(\"\"),\n",
    "        f\"precision_vs_quality_{int(time.time()) % 1000000:06}.png\"\n",
    "    )\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850bc449-925b-4334-99a2-85905557fc5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
