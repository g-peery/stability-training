{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41034072-4b51-40ec-b16e-5c3b601a69b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ResNet Notebook\n",
    "\n",
    "This notebook is for reproducing the paper with ResNet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4054fe80-6332-4e58-a0c9-6338ceb6f191",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Dependencies\n",
    "\n",
    "Let's setup and configure dependencies. Note that since torchviz is used, graphviz needs to be specially installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ee8ce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%matplotlib widget\n",
    "\n",
    "import contextlib\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import random\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.io import ImageReadMode\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms as T\n",
    "from torchviz import make_dot # This requires external downloads as well\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1d91ed-5a91-4f62-bc7f-31bfa59a65bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "def seed_worker(worker_id):\n",
    "    \"\"\"\n",
    "    Sets the seed of the worker to depend on the initial seed. Credit to\n",
    "    https://pytorch.org/docs/stable/notes/randomness.html\n",
    "    \"\"\"\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    \n",
    "torch.manual_seed(0)\n",
    "# For DataLoaders\n",
    "g = torch.Generator().manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2045b5-74a7-41b8-b0be-2da5d6e6dfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare progress bar\n",
    "tqdm_variant = tqdm.tqdm # Assume not in Jupyter\n",
    "try:\n",
    "    # Credit https://stackoverflow.com/a/39662359\n",
    "    shell_name = get_ipython().__class__.__name__\n",
    "    if shell_name == \"ZMQInteractiveShell\":\n",
    "        # Case in Jupyter\n",
    "        tqdm_variant = tqdm.notebook.tqdm\n",
    "except NameError:\n",
    "    # Probably no iPhython instance, just use standard\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6e58de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11be810b-db52-466b-a73a-030dec6214b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Plotting and Debugging Tools\n",
    "\n",
    "Let's create some helpful functions for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e861da12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from Steven's ModelExamples\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.gca().get_xaxis().set_visible(False)\n",
    "    plt.gca().get_yaxis().set_visible(False)\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af008e45-5924-4d4b-b859-9b3701a50427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_batch_sample(loader):\n",
    "    dataiter = iter(loader)\n",
    "    imgs, labels = next(dataiter)\n",
    "    img_grid = torchvision.utils.make_grid(imgs)\n",
    "    imgs, labels = imgs.to(device), labels.to(device)\n",
    "    matplotlib_imshow(img_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57feb0c1-18ca-4346-a620-a26030a7877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cumulative_dist(\n",
    "    baseline_observations,\n",
    "    stabilized_observations\n",
    "):\n",
    "    \"\"\"Plot our reproduction of Figure 7.\"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(\"Cumulative Distributions\")\n",
    "    ax.set_xlabel(\"Feature Distance\")\n",
    "    ax.set_ylabel(\"Cumulative Fraction\")\n",
    "    ax.hist(\n",
    "        baseline_observations,\n",
    "        round(np.sqrt(len(baseline_observations))),\n",
    "        density=True,\n",
    "        histtype=\"step\",\n",
    "        cumulative=True,\n",
    "        label=\"Baseline\"\n",
    "    )\n",
    "    ax.hist(\n",
    "        stabilized_observations,\n",
    "        round(np.sqrt(len(stabilized_observations))),\n",
    "        density=True,\n",
    "        histtype=\"step\",\n",
    "        cumulative=True,\n",
    "        label=\"Stabilized\"\n",
    "    )\n",
    "    ax.legend()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17edab80-3391-4fd6-977d-635f89910893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_first_triplet(queries, positives, negatives):\n",
    "    a, b, c = queries[0], positives[0], negatives[0]\n",
    "    ToPIL = T.ToPILImage() # Converting function\n",
    "    fig, ax = plt.subplots(1, 3)\n",
    "    ax[0].imshow(ToPIL(a))\n",
    "    ax[1].imshow(ToPIL(b))\n",
    "    ax[2].imshow(ToPIL(c))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19f07a2d-02bf-426b-abe0-d4e319553a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_vs_quality(\n",
    "    qualities,\n",
    "    baseline_precision,\n",
    "    stabilized_precision\n",
    "):\n",
    "    \"\"\"Plots precision vs quality as in Figure 9.\"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(\"Precision vs Quality\")\n",
    "    ax.set_xlabel(\"JPEG Quality\")\n",
    "    ax.set_ylabel(\"Precision@top-1\")\n",
    "    plt.plot(qualities, baseline_precision, label=\"Baseline\")\n",
    "    plt.plot(qualities, stabilized_precision, label=\"Stabilized\")\n",
    "    ax.legend()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64bfaafa-c819-4b54-9609-11bdec012522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_distances(triplet_ranking_model, batch, item_idx=0):\n",
    "    with torch.no_grad():\n",
    "        output = triplet_ranking_model(*map(lambda x : x.to(device), batch))\n",
    "        qfeat = output[item_idx][0]\n",
    "        pfeat = output[item_idx][1]\n",
    "        nfeat = output[item_idx][2]\n",
    "        pos_dist = torch.sqrt(((qfeat - pfeat)**2).sum()).item()\n",
    "        neg_dist = torch.sqrt(((qfeat - nfeat)**2).sum()).item()\n",
    "        print(f\"Distance to positive: {pos_dist}, to negative: {neg_dist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc57be1-7ce4-4ee2-8a9d-cda6b6e0642d",
   "metadata": {},
   "source": [
    "This part is also helpful for storing results of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab9f248-96b3-43bc-bf1a-7e45d4c33a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _def_save_filename(iteration_number : int, is_model : bool):\n",
    "    \"\"\"\n",
    "    Returns a save filename from an iteration number, and whether the\n",
    "    thing being saved is the model or the loss progress.\n",
    "    \"\"\"\n",
    "    # Make sure inputs are okay (no directory traversal attacks!)\n",
    "    if not isinstance(iteration_number, int):\n",
    "        raise TypeError(\"Iteration number should be an integer.\")\n",
    "\n",
    "    # Retrieve proper name\n",
    "    if is_model:\n",
    "        return os.path.join(\n",
    "            os.path.dirname(os.path.abspath(\"\")),\n",
    "            f\"model_save_{iteration_number}_{int(time.time())%1000000:06}.pt\"\n",
    "        )\n",
    "    return os.path.join(\n",
    "        os.path.dirname(os.path.abspath(\"\")),\n",
    "        f\"progress_save_{iteration_number}_{int(time.time())%1000000:06}.pt\"\n",
    "    )\n",
    "\n",
    "#\n",
    "# This class will make recording information easier\n",
    "#\n",
    "class TrainResult:\n",
    "    \"\"\"\n",
    "    Hold the results of a training round. Basically a nice wrapper.\n",
    "\n",
    "    Losses and accuracies should be averages over a batch for plot\n",
    "    labels to make sense.\n",
    "\n",
    "    Use whichever storage makes sense, but if one doesn't use the\n",
    "    storage then don't expect the corresponding plots to do anything.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        \"\"\"\n",
    "        Constructor - prepare local variables. Include model since the\n",
    "        one provided to trainer will likely be moved to a different\n",
    "        device. Provide the one that is actually trained here, and then\n",
    "        it can be used.\n",
    "        \"\"\"\n",
    "        self.train_loss_history = []\n",
    "        self.val_loss_history = []\n",
    "        self.train_accuracy_history = []\n",
    "        self.val_accuracy_history = []\n",
    "        self.time_training = 0.0\n",
    "        self.time_validating = 0.0\n",
    "        self.model = model\n",
    "\n",
    "    def plot_loss_train_valid_curves(self, ax, show_legend : bool=True):\n",
    "        \"\"\"Use matplotlib to show training curves. Supply axis in ax.\"\"\"\n",
    "        ax.plot(self.train_loss_history, label=\"Train\")\n",
    "        ax.plot(self.val_loss_history, label=\"Test\")\n",
    "        ax.title(f\"Model Loss\")\n",
    "        ax.xlabel(\"Iteration\")\n",
    "        ax.ylabel(\"Loss\")\n",
    "        if show_legend:\n",
    "            ax.legend()\n",
    "\n",
    "    def plot_accuracy_train_valid_curves(self, ax, show_legend=True):\n",
    "        \"\"\"Use matplotlib to show training curves. Supply axis in ax.\"\"\"\n",
    "        ax.plot(self.train_accuracy_history, label=\"Train\")\n",
    "        ax.plot(self.val_accuracy_history, label=\"Test\")\n",
    "        ax.title(f\"Model Accuracy\")\n",
    "        ax.xlabel(\"Iteration\")\n",
    "        ax.ylabel(\"Accuracy\")\n",
    "        if show_legend:\n",
    "            ax.legend()\n",
    "\n",
    "    def print_time_info(self):\n",
    "        \"\"\"Prints some lines with information about timing.\"\"\"\n",
    "        print(f\"Spent {round(self.time_training)} seconds training.\")\n",
    "        print(f\"Spent {round(self.time_validating)} seconds evaluating.\")\n",
    "\n",
    "    def full_analysis(self, axs):\n",
    "        \"\"\"Display all analysis plots and print time information.\"\"\"\n",
    "        if len(self.train_loss_history) > 0:\n",
    "            self.plot_loss_train_valid_curves(axs[0])\n",
    "        if len(self.train_accuracy_history) > 0:\n",
    "            self.plot_accuracy_train_valid_curves(ax[1])\n",
    "        if self.time_training > 0.0:\n",
    "            self.print_time_info()\n",
    "\n",
    "    def save(self, save_filename_func, count : int=0):\n",
    "        \"\"\"\n",
    "        Uses the save_filename_func provided with the count to create\n",
    "        files that save the models current weights and the training\n",
    "        progress so far.\n",
    "\n",
    "        For count, 0 is by convention the final version.\n",
    "        \"\"\"\n",
    "        # Save model\n",
    "        torch.save(self.model.state_dict(), save_filename_func(count, True))\n",
    "        # Save progress\n",
    "        with open(save_filename_func(count, False), \"wb\") as file_obj:\n",
    "            pickle.dump(\n",
    "                {\n",
    "                    \"train_loss_history\" : self.train_loss_history,\n",
    "                    \"val_loss_history\" : self.val_loss_history,\n",
    "                    \"train_accuracy_history\" : self.train_accuracy_history,\n",
    "                    \"val_accuracy_history\" : self.val_accuracy_history,\n",
    "                    \"time_training\" : self.time_training,\n",
    "                    \"time_validating\" : self.time_validating\n",
    "                }, file_obj\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4a7ec4f1-79b6-4814-b5d1-d705dfffd422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 3.0.0 (20220226.1711)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"397pt\" height=\"281pt\"\n",
       " viewBox=\"0.00 0.00 397.00 281.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 277)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-277 393,-277 393,4 -4,4\"/>\n",
       "<!-- 2614144931216 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>2614144931216</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"109,-31 38,-31 38,0 109,0 109,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"73.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (32, 64)</text>\n",
       "</g>\n",
       "<!-- 2607197291856 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>2607197291856</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"124,-86 23,-86 23,-67 124,-67 124,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"73.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 2607197291856&#45;&gt;2614144931216 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>2607197291856&#45;&gt;2614144931216</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M73.5,-66.79C73.5,-60.07 73.5,-50.4 73.5,-41.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"77,-41.19 73.5,-31.19 70,-41.19 77,-41.19\"/>\n",
       "</g>\n",
       "<!-- 2607197292048 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2607197292048</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"196,-141 95,-141 95,-122 196,-122 196,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"145.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 2607197292048&#45;&gt;2607197291856 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>2607197292048&#45;&gt;2607197291856</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M133.93,-121.98C123.08,-114 106.62,-101.88 93.62,-92.31\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"95.41,-89.28 85.28,-86.17 91.26,-94.92 95.41,-89.28\"/>\n",
       "</g>\n",
       "<!-- 2613489619152 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>2613489619152</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"261,-86 160,-86 160,-67 261,-67 261,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"210.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 2607197292048&#45;&gt;2613489619152 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>2607197292048&#45;&gt;2613489619152</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M155.94,-121.98C165.65,-114.07 180.32,-102.11 192,-92.58\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"194.33,-95.2 199.87,-86.17 189.9,-89.78 194.33,-95.2\"/>\n",
       "</g>\n",
       "<!-- 2613489620256 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>2613489620256</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"389,-86 288,-86 288,-67 389,-67 389,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"338.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 2607197292048&#45;&gt;2613489620256 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>2607197292048&#45;&gt;2613489620256</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M176.51,-121.98C209.33,-112.97 261.3,-98.7 297.49,-88.76\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"298.53,-92.11 307.25,-86.08 296.68,-85.36 298.53,-92.11\"/>\n",
       "</g>\n",
       "<!-- 2614149455360 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2614149455360</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"188,-207 93,-207 93,-177 188,-177 188,-207\"/>\n",
       "<text text-anchor=\"middle\" x=\"140.5\" y=\"-195\" font-family=\"monospace\" font-size=\"10.00\">model.fc.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"140.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 2614149455360&#45;&gt;2607197292048 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>2614149455360&#45;&gt;2607197292048</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M141.71,-176.84C142.36,-169.21 143.17,-159.7 143.88,-151.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"147.39,-151.53 144.75,-141.27 140.41,-150.93 147.39,-151.53\"/>\n",
       "</g>\n",
       "<!-- 2607197290752 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>2607197290752</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"77,-141 0,-141 0,-122 77,-122 77,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"38.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 2607197290752&#45;&gt;2607197291856 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2607197290752&#45;&gt;2607197291856</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M44.28,-121.75C49.12,-114.42 56.18,-103.73 62.14,-94.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"65.24,-96.36 67.83,-86.09 59.4,-92.5 65.24,-96.36\"/>\n",
       "</g>\n",
       "<!-- 2613489620736 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>2613489620736</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"307,-201.5 206,-201.5 206,-182.5 307,-182.5 307,-201.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"256.5\" y=\"-189.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 2613489620736&#45;&gt;2607197290752 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2613489620736&#45;&gt;2607197290752</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M224.8,-182.49C186.54,-172.23 122.09,-154.93 79.92,-143.62\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"80.8,-140.23 70.24,-141.02 78.99,-146.99 80.8,-140.23\"/>\n",
       "</g>\n",
       "<!-- 2613489621888 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>2613489621888</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"291,-141 214,-141 214,-122 291,-122 291,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"252.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 2613489620736&#45;&gt;2613489621888 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>2613489620736&#45;&gt;2613489621888</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M255.91,-182.37C255.35,-174.25 254.5,-161.81 253.79,-151.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"257.27,-150.91 253.09,-141.17 250.28,-151.38 257.27,-150.91\"/>\n",
       "</g>\n",
       "<!-- 2613489620688 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>2613489620688</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"386,-141 309,-141 309,-122 386,-122 386,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"347.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 2613489620736&#45;&gt;2613489620688 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>2613489620736&#45;&gt;2613489620688</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M269.93,-182.37C284.58,-172.95 308.26,-157.73 325.6,-146.58\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"327.5,-149.52 334.02,-141.17 323.71,-143.63 327.5,-149.52\"/>\n",
       "</g>\n",
       "<!-- 2612142805936 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>2612142805936</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"310,-273 203,-273 203,-243 310,-243 310,-273\"/>\n",
       "<text text-anchor=\"middle\" x=\"256.5\" y=\"-261\" font-family=\"monospace\" font-size=\"10.00\">model.fc.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"256.5\" y=\"-250\" font-family=\"monospace\" font-size=\"10.00\"> (64, 512)</text>\n",
       "</g>\n",
       "<!-- 2612142805936&#45;&gt;2613489620736 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>2612142805936&#45;&gt;2613489620736</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M256.5,-242.8C256.5,-233.7 256.5,-221.79 256.5,-211.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"260,-211.84 256.5,-201.84 253,-211.84 260,-211.84\"/>\n",
       "</g>\n",
       "<!-- 2614144932896 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>2614144932896</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"246,-31 175,-31 175,0 246,0 246,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"210.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (32, 64)</text>\n",
       "</g>\n",
       "<!-- 2613489619152&#45;&gt;2614144932896 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>2613489619152&#45;&gt;2614144932896</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M210.5,-66.79C210.5,-60.07 210.5,-50.4 210.5,-41.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"214,-41.19 210.5,-31.19 207,-41.19 214,-41.19\"/>\n",
       "</g>\n",
       "<!-- 2613489621888&#45;&gt;2613489619152 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>2613489621888&#45;&gt;2613489619152</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M245.56,-121.75C239.63,-114.26 230.93,-103.28 223.68,-94.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"226.26,-91.75 217.31,-86.09 220.77,-96.1 226.26,-91.75\"/>\n",
       "</g>\n",
       "<!-- 2614144930896 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>2614144930896</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"374,-31 303,-31 303,0 374,0 374,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"338.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (32, 64)</text>\n",
       "</g>\n",
       "<!-- 2613489620256&#45;&gt;2614144930896 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>2613489620256&#45;&gt;2614144930896</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M338.5,-66.79C338.5,-60.07 338.5,-50.4 338.5,-41.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"342,-41.19 338.5,-31.19 335,-41.19 342,-41.19\"/>\n",
       "</g>\n",
       "<!-- 2613489620688&#45;&gt;2613489620256 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>2613489620688&#45;&gt;2613489620256</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M346.01,-121.75C344.83,-114.8 343.14,-104.85 341.66,-96.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"345.08,-95.36 339.96,-86.09 338.18,-96.53 345.08,-95.36\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x25f091185b0>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient_sanity_display(model, input_iter):\n",
    "    \"\"\"\n",
    "    Show a computational graph for a model on device given an iterable\n",
    "    of inputs to provide to the model.\n",
    "    \"\"\"\n",
    "    res = model(*[item.to(device) for item in input_iter])\n",
    "    make_dot(res, params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2bc7b5-3ab6-4314-adc5-78d3d976254e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Metrics\n",
    "\n",
    "Let's prepare some evaluation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90f9c48b-17ea-4c4e-967f-2c2b8e8e132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_get_accuracy(model_output, labels):\n",
    "    \"\"\"\n",
    "    Given model output and labels, finds the average accuracy over the \n",
    "    batch.\n",
    "    \"\"\"\n",
    "    preds = model_output.topk(1, dim=1)[1].t().flatten()\n",
    "    return (preds == labels).sum() / len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e999aef-7d29-4c15-8ab4-5bdb530e9dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_classifier_get_accuracy(model_output, labels):\n",
    "    \"\"\"For stability trained classifier, finds the accuracy\"\"\"\n",
    "    return (\n",
    "        torch.sum(\n",
    "            torch.split(\n",
    "                model_output, CLASS_NUMBER, 1\n",
    "            )[0].topk(1, dim=1)[1].t().flatten()\n",
    "            == labels\n",
    "        )\n",
    "        / len(model_output)\n",
    "   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e451b31-983c-4cb5-9613-beb1aa72330e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Experiments\n",
    "\n",
    "Let's make some functions to run experiments on models.\n",
    "\n",
    "For the following, see how closely the triplet ranking models think compressed versions of the same images are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f5e7fb-1544-4321-bba1-49c814249271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_observations(baseline_model, stabilized_model, imagefolder):\n",
    "    \"\"\"\n",
    "    Given a model trained for triplet ranking, make duplicate detection\n",
    "    observations.\n",
    "    \"\"\"\n",
    "    baseline_observations = []\n",
    "    stabilized_observations = []\n",
    "    \n",
    "    for img_name in os.listdir(imagefolder):\n",
    "        #\n",
    "        # Deal with images\n",
    "        #\n",
    "        original_img = Image.open(image_fpath)\n",
    "        # Thanks https://stackoverflow.com/a/30771751\n",
    "        buffer = StringIO.StringIO()\n",
    "        original_img.save(buffer, \"JPEG\", optimize=True, quality=50)\n",
    "        compressed_img = Image.open(buffer)\n",
    "        \n",
    "        orig_tensor = T.ToTensor(original_img)\n",
    "        comp_tensor = T.ToTensor(compressed_img)\n",
    "        #\n",
    "        # Make observations\n",
    "        #\n",
    "        baseline_model.eval()\n",
    "        stabilized_model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Baseline\n",
    "            original_rep = baseline_model(orig_tensor)\n",
    "            compressed_rep = baseline_model(comp_tensor)\n",
    "            dist = torch.sqrt(((original_rep - compressed_rep)**2).sum())\n",
    "            baseline_observations.append(dist)\n",
    "            # Stabilized\n",
    "            original_rep = stabilized_model(orig_tensor)\n",
    "            compressed_rep = stabilized_model(comp_tensor)\n",
    "            dist = torch.sqrt(((original_rep - compressed_rep)**2).sum())\n",
    "            stabilized_observations.append(dist)\n",
    "            \n",
    "    return baseline_observations, stabilized_observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671bb892-d181-4112-a463-914b8324e206",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Datasets\n",
    "\n",
    "Let's prepare some datasets. First, we'll make a custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4cedeaca-3215-46cb-a183-ce1b8b1c2dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: an alternative to this paradigm is loading all the images using\n",
    "# the transform as a catalogue onto main memory, then detach on access\n",
    "# TODO - refactor\n",
    "class TripletRankingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 img_dir : str,\n",
    "                 pos_per_img : int,\n",
    "                 neg_per_pos : int,\n",
    "                 transform=None,\n",
    "                 training=True\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Point at directory which has subdirectories corresponding to\n",
    "        each class. Supply the number of positive images per image in\n",
    "        the subdirectories and the number of negative images to combine\n",
    "        with.\n",
    "        \"\"\"\n",
    "        if transform is None:\n",
    "            transform = nn.Identity()\n",
    "        self.transform = transform\n",
    "        self.mode = \"RGB\"#ImageReadMode.RGB\n",
    "        \n",
    "        #\n",
    "        # Initialize triplets\n",
    "        #\n",
    "        self.triplets = [ ]\n",
    "        \n",
    "        if training:\n",
    "            self._train_init(img_dir, pos_per_img, neg_per_pos)\n",
    "        else:\n",
    "            self._val_init(img_dir, pos_per_img, neg_per_pos)\n",
    "        \n",
    "    def _train_init(self, img_dir, pos_per_img, neg_per_pos):\n",
    "        # Generate randoms first, index into later. Need to count\n",
    "        # Iterate over classes\n",
    "        class_names = os.listdir(img_dir)\n",
    "        if os.path.exists(os.path.join(img_dir, class_names[0], \"images\")):\n",
    "            subdirectories_paths = list(map(\n",
    "                lambda x : os.path.join(img_dir, x, \"images\"),\n",
    "                class_names\n",
    "            ))\n",
    "        else:\n",
    "            subdirectories_paths = list(map(\n",
    "                lambda x : os.path.join(img_dir, x),\n",
    "                class_names\n",
    "            ))\n",
    "        for subdir_path in subdirectories_paths:\n",
    "            img_per_class = len(os.listdir(subdir_path))\n",
    "            break\n",
    "        randoms = np.random.randint(\n",
    "            0,\n",
    "            img_per_class,\n",
    "            (\n",
    "                len(subdirectories_paths),\n",
    "                img_per_class,\n",
    "                pos_per_img,\n",
    "                neg_per_pos + 1 # Last for positive selection\n",
    "            ),\n",
    "            dtype=\"int16\"\n",
    "        )\n",
    "        \n",
    "        # TODO - this method inefficient, only need to hold 2 numbers,\n",
    "        # some string format info\n",
    "        # DANGER - potential for loops with symbolic/hard links\n",
    "        # Iterate over classes\n",
    "        for subdir_idx, subdir_path in enumerate(subdirectories_paths):\n",
    "            exclude_subdir_idx =  [\n",
    "                            *range(subdir_idx),\n",
    "                            *range(\n",
    "                                subdir_idx + 1,\n",
    "                                len(subdirectories_paths)\n",
    "                            )\n",
    "                        ]\n",
    "            # Iterate over images\n",
    "            for img_idx, img in enumerate(os.listdir(subdir_path)):\n",
    "                img_path = os.path.join(subdir_path, img)\n",
    "                # Iterate over alike images\n",
    "                for pos_it in range(pos_per_img):\n",
    "                    pos_rand = randoms[subdir_idx][img_idx][pos_it][-1]\n",
    "                    # Adjust on equal\n",
    "                    if pos_rand == pos_it:\n",
    "                        if pos_rand == 0:\n",
    "                            pos_rand += 1\n",
    "                        else:\n",
    "                            pos_rand -= 1\n",
    "                    # Form name\n",
    "                    pos_path = os.path.join(\n",
    "                        subdir_path,\n",
    "                        f\"{class_names[subdir_idx]}_{pos_rand}.JPEG\"\n",
    "                    )\n",
    "                    # Need more randoms for next step\n",
    "                    neg_class_rands = np.random.choice(\n",
    "                        exclude_subdir_idx,\n",
    "                        neg_per_pos,\n",
    "                        replace=True\n",
    "                    )\n",
    "                    # Iterate over dissimilar images\n",
    "                    for neg_it in range(neg_per_pos):\n",
    "                        class_rand = neg_class_rands[neg_it]\n",
    "                        neg_class_path = subdirectories_paths[class_rand]\n",
    "                        neg_class = class_names[class_rand]\n",
    "                        neg_rand = randoms[subdir_idx][img_idx][pos_it][neg_it]\n",
    "                        # Form name\n",
    "                        neg_path = os.path.join(\n",
    "                            neg_class_path,\n",
    "                            f\"{neg_class}_{neg_rand}.JPEG\"\n",
    "                        )\n",
    "                        # Append\n",
    "                        self.triplets.append((img_path, pos_path, neg_path))\n",
    "                        \n",
    "    def _val_init(self, img_dir, pos_per_img, neg_per_pos):\n",
    "        # Generate randoms first, index into later. Need to count\n",
    "        # Iterate over classes\n",
    "        class_names = os.listdir(img_dir)\n",
    "        \n",
    "        if os.path.exists(os.path.join(img_dir, class_names[0], \"images\")):\n",
    "            subdirectories_paths = list(map(\n",
    "                lambda x : os.path.join(img_dir, x, \"images\"),\n",
    "                class_names\n",
    "            ))\n",
    "        else:\n",
    "            subdirectories_paths = list(map(\n",
    "                lambda x : os.path.join(img_dir, x),\n",
    "                class_names\n",
    "            ))\n",
    "            \n",
    "        img_per_class = dict()\n",
    "        img_names = []\n",
    "        for idx in range(len(class_names)):\n",
    "            subdir_path = subdirectories_paths[idx]\n",
    "            class_name = class_names[idx]\n",
    "            img_names.append(list(os.listdir(subdir_path)))\n",
    "            img_per_class[class_name] = len(img_names[-1])\n",
    "        \n",
    "        # TODO - this method inefficient, only need to hold 2 numbers,\n",
    "        # some string format info\n",
    "        # DANGER - potential for loops with symbolic/hard links\n",
    "        # Iterate over classes\n",
    "        for subdir_idx, subdir_path in enumerate(subdirectories_paths):\n",
    "            exclude_subdir_idx =  [\n",
    "                            *range(subdir_idx),\n",
    "                            *range(\n",
    "                                subdir_idx + 1,\n",
    "                                len(subdirectories_paths)\n",
    "                            )\n",
    "                        ]\n",
    "            this_dir = img_names[subdir_idx]\n",
    "            # Iterate over images\n",
    "            for img_idx, img in enumerate(this_dir):\n",
    "                img_path = os.path.join(subdir_path, img)\n",
    "                # Iterate over alike images\n",
    "                for pos_it in range(pos_per_img):\n",
    "                    # Form name\n",
    "                    rand_idx = np.random.choice([\n",
    "                        *range(pos_it),\n",
    "                        *range(pos_it + 1, img_per_class[class_names[pos_it]])\n",
    "                    ])\n",
    "                    pos_name = this_dir[rand_idx]\n",
    "                    # Form path\n",
    "                    pos_path = os.path.join(subdir_path, pos_name)\n",
    "                    # Need more randoms for next step\n",
    "                    neg_class_rands = np.random.choice(\n",
    "                        exclude_subdir_idx,\n",
    "                        neg_per_pos,\n",
    "                        replace=True\n",
    "                    )\n",
    "                    # Iterate over dissimilar images\n",
    "                    for neg_it in range(neg_per_pos):\n",
    "                        class_rand = neg_class_rands[neg_it]\n",
    "                        neg_class_path = subdirectories_paths[class_rand]\n",
    "                        neg_class = class_names[class_rand]\n",
    "                        # Select from the directory\n",
    "                        neg_name = np.random.choice(img_names[class_rand])\n",
    "                        # Form name\n",
    "                        neg_path = os.path.join(neg_class_path, neg_name)\n",
    "                        # Append\n",
    "                        self.triplets.append((img_path, pos_path, neg_path))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        this_triplet = self.triplets[idx]\n",
    "        #query = self.transform(read_image(this_triplet[0], self.mode))\n",
    "        #positive = self.transform(read_image(this_triplet[1], self.mode))\n",
    "        #negative = self.transform(read_image(this_triplet[2], self.mode))\n",
    "        query = self.transform(Image.open(this_triplet[0]).convert(self.mode))\n",
    "        positive = self.transform(Image.open(this_triplet[1]).convert(self.mode))\n",
    "        negative = self.transform(Image.open(this_triplet[2]).convert(self.mode))\n",
    "        return query, positive, negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3597933-c598-4edf-9d96-55a37a7ebf09",
   "metadata": {},
   "source": [
    "The following is helpful to keep around:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86997b7d-71ce-4e6f-9a5e-ef0725ee3271",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.path.abspath(\"\"), \"tiny-imagenet-200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28c8b26-39c0-4500-8b21-2c2b8f346b2a",
   "metadata": {},
   "source": [
    "We'll create dataset objects as we go since they take up memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e225f7-ffcc-44a3-8164-a3ba84e1f152",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Models\n",
    "\n",
    "We'll similarly create model objects as we go along, but this section contains things that simplify making them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ccb2b3-0eea-49dc-b080-9632eda8b6ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Helper Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968864a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit https://discuss.pytorch.org/t/how-to-add-noise-to-mnist-dataset-when-using-pytorch/59745\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return (\n",
    "            tensor\n",
    "            + torch.empty(tensor.size(), device=device).normal_() * self.std\n",
    "            + self.mean\n",
    "        )\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            self.__class__.__name__\n",
    "            + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad20f6a0-d46f-4ee0-9539-27a914d4ab74",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "01a52b46-8c49-4959-8f83-39a3851cad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletRanker(nn.Module):\n",
    "    \"\"\"Triplet ranking model.\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        \"\"\"Creates a triplet ranker out of the model.\"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, queries, positives, negatives):\n",
    "        \"\"\"Send batch of triplets through.\"\"\"\n",
    "        queries_encode = self.model(queries)\n",
    "        positives_encode = self.model(positives)\n",
    "        negatives_encode = self.model(negatives)\n",
    "        return queries_encode, positives_encode, negatives_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18b078d-5644-4d88-8ca2-7037dd224c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StabilityTrainingModel(torch.nn.Module):\n",
    "    def __init__(self, other_model, perturbation=None):\n",
    "        \"\"\"\n",
    "        Wraps another model to allow for stability training. May specify\n",
    "        custom perturbation transformation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = other_model\n",
    "        \n",
    "        if perturbation is None:\n",
    "            self.perturb = AddGaussianNoise(0, 0.04)\n",
    "        else:\n",
    "            self.perturb = perturbation\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Apply the model to some input.\"\"\"\n",
    "        return self.model(x), self.model(self.perturb(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74efcfb-4e9d-4a70-9f55-4c9626ae1cb4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b11a144-870a-4028-95b8-9d6b3e430ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will act like a class\n",
    "def RobustResnet():\n",
    "    \"\"\"\n",
    "    Retrieve a fresh ResNet ready for stability training and fine\n",
    "    tuning.\n",
    "    \"\"\"\n",
    "    bare_resnet = torchvision.models.resnet18(pretrained=True)\n",
    "    for param in bare_resnet.parameters():\n",
    "        param.requires_grad = False\n",
    "    bare_resnet.fc = nn.Linear(512, CLASS_NUMBER, bias=True)\n",
    "    \n",
    "    return StabilityTrainingModel(bare_resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c7da55-0c63-4406-8cdd-0daf01d7dc97",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Training Functions\n",
    "\n",
    "Factoring some common patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1e10a4e4-c9ab-457a-818d-d853f38b31a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_epoch(\n",
    "    model,\n",
    "    loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    loss_func,\n",
    "    result,\n",
    "    training=True\n",
    "):\n",
    "    loss = torch.Tensor([0])\n",
    "    avg_loss = 0\n",
    "    \n",
    "    if training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    start = time.perf_counter()\n",
    "    with contextlib.ExitStack() as stack: # For validation/training\n",
    "        if not training:\n",
    "            stack.enter_context(torch.no_grad()) # No gradients\n",
    "        for imgs, labels in tqdm_variant(\n",
    "            loader,\n",
    "            desc=f\"{'Training' if training else 'Validation'} Iteration\",\n",
    "            disable=False\n",
    "        ):\n",
    "            if training:\n",
    "                # Prepare optimizer\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Prepare relevant variables\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            out = model(imgs)\n",
    "\n",
    "            # Determine loss\n",
    "            loss = loss_func(out, labels)\n",
    "\n",
    "            # Store intermediate results - note average over batch\n",
    "            item = loss.item()\n",
    "            avg_loss += item\n",
    "            if training:\n",
    "                result.train_loss_history.append(item)\n",
    "            else:\n",
    "                result.val_loss_history.append(item)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            acc = (\n",
    "                torch.sum(\n",
    "                    out.topk(1, dim=1)[1].t().flatten() == labels\n",
    "                ) / len(out)\n",
    "            ).item()\n",
    "            if training:\n",
    "                result.train_accuracy_history.append(acc)\n",
    "            else:\n",
    "                result.val_accuracy_history.append(acc)\n",
    "\n",
    "            if training:\n",
    "                loss.backward() # Get gradients\n",
    "                optimizer.step() # Descend gradients\n",
    "    end = time.perf_counter()\n",
    "    if training:\n",
    "        result.time_training += end - start\n",
    "    else:\n",
    "        result.time_validating += end - start\n",
    "    \n",
    "    return avg_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77288ff1-6e0f-43e2-9780-e3e94312e819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_epoch(\n",
    "    model,\n",
    "    loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    loss_func,\n",
    "    result,\n",
    "    training=True\n",
    "):\n",
    "    loss = torch.Tensor([0])\n",
    "    avg_loss = 0\n",
    "    \n",
    "    if training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    start = time.perf_counter()\n",
    "    with contextlib.ExitStack() as stack: # For validation/training\n",
    "        if not training:\n",
    "            stack.enter_context(torch.no_grad()) # No gradients\n",
    "        for queries, positives, negatives in tqdm_variant(\n",
    "            loader,\n",
    "            desc=f\"{'Training' if training else 'Validation'} Iteration\",\n",
    "            disable=False\n",
    "        ):\n",
    "            if training:\n",
    "                # Prepare optimizer\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Prepare relevant variables\n",
    "            queries = queries.to(device)\n",
    "            positives = positives.to(device)\n",
    "            negatives = negatives.to(device)\n",
    "            out = model(queries, positives, negatives)\n",
    "\n",
    "            # Determine loss\n",
    "            loss = loss_func(*out)\n",
    "\n",
    "            # Store intermediate results - note average over batch\n",
    "            item = loss.item()\n",
    "            avg_loss += item\n",
    "            if training:\n",
    "                result.train_loss_history.append(item)\n",
    "            else:\n",
    "                result.val_loss_history.append(item)\n",
    "\n",
    "            if training:\n",
    "                loss.backward() # Get gradients\n",
    "                optimizer.step() # Descend gradients\n",
    "    end = time.perf_counter()\n",
    "    if training:\n",
    "        result.time_training += end - start\n",
    "    else:\n",
    "        result.time_validating += end - start\n",
    "    \n",
    "    return avg_loss / len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce64b9b-dca7-4fd0-b015-65011d31d691",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Miscellaneous Helpers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54635cd8-2907-402c-966b-2cd603a47396",
   "metadata": {},
   "source": [
    "Some constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e03d5a6b-2e35-468c-80c1-a84c55c0de08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight of the stability objective\n",
    "ALPHA = 0.01\n",
    "\n",
    "CLASS_NUMBER = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6658653-1a46-45dc-8811-6864122d7697",
   "metadata": {},
   "source": [
    "Loss function things for stability trained classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ad874a9-2422-4730-aa06-4280b1dc46df",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_objective = nn.CrossEntropyLoss()\n",
    "classifier_stability_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "def full_loss(clean_output, distort_output, label):\n",
    "    return (\n",
    "        training_objective(clean_output, label)\n",
    "        + ALPHA * classifier_stability_loss(distort_output, clean_output)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4ed5d1-87f5-4f24-a08f-955e19bb425b",
   "metadata": {},
   "source": [
    "ResNet is made to be used with data transformed as so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74886e62-5e7a-42b2-84fd-3f43dccac3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_transform = T.Compose([\n",
    "    T.Resize((256,256)),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798010ca-f022-41e7-9b9a-7c4ed0beefcc",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now, let's do some training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9a84a5-bf45-4618-8052-a7ed3c1ae52b",
   "metadata": {},
   "source": [
    "### Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed259b-09fc-454e-862a-99b90a81a645",
   "metadata": {},
   "source": [
    "#### Baseline\n",
    "\n",
    "Let's train a baseline ResNet. First, prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d71b6f73-230b-45b2-8cf7-57edd10fd793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dataset_train = datasets.ImageFolder(\n",
    "    os.path.join(data_dir, \"train\"),\n",
    "    transform=resnet_transform\n",
    ")\n",
    "class_dataset_val = datasets.ImageFolder(\n",
    "    os.path.join(data_dir, \"val\"),\n",
    "    transform=resnet_transform\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    class_dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    class_dataset_val,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634133e2-ffbb-40ea-a6cb-83dc522bcac7",
   "metadata": {},
   "source": [
    "Next, prepare the model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3ea6b25-5db6-451e-bbe6-93047a5d1f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default variation. Note pretrained\n",
    "resnet = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all layers\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Replace last, note now unfrozen and we'll fine tune\n",
    "resnet.fc = nn.Linear(512, 200, bias=True)\n",
    "\n",
    "# Send to GPU, if possible\n",
    "resnet = resnet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03e3211-f51b-4b6a-9324-c4fb48c15b4e",
   "metadata": {},
   "source": [
    "Learning controllers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "166c6ebc-e8b3-4ee7-9a27-a237eabca507",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(resnet.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    threshold=0.1\n",
    ")\n",
    "# May try OneCycleLR, annealers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d9e6a6-850c-4996-8857-4bc82de6ba29",
   "metadata": {},
   "source": [
    "The loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "731a21ec-4940-4ce9-954f-80ab7bf8a1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_loss = classifier_objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293e28ce-565a-49ad-8925-04eef82fa8b2",
   "metadata": {},
   "source": [
    "Keep the result in here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c35154fa-52ad-4e12-84b5-32bae925ab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_result = TrainResult(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ae5aa8-6049-48fe-9122-2894bbb2435f",
   "metadata": {},
   "source": [
    "The actual training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1771747e-f693-4e77-9cd2-1e61881aea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for some number of epochs\n",
    "EPOCH_COUNT = 1#20\n",
    "SAVE_PERIOD = 3\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "best_count = 0\n",
    "for epoch in tqdm_variant(\n",
    "    range(EPOCH_COUNT),\n",
    "    desc=f\"Epoch\",\n",
    "    unit=\"epoch\",\n",
    "    disable=False\n",
    "):\n",
    "    # Debug learning rate\n",
    "    print(f\"Current Learning Rate: {optimizer.param_groups[0]['lr']}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = classifier_epoch(\n",
    "        resnet,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        device,\n",
    "        resnet_loss,\n",
    "        resnet_result\n",
    "    )\n",
    "\n",
    "    # Validation\n",
    "    val_loss = classifier_epoch(\n",
    "        resnet,\n",
    "        val_loader,\n",
    "        device,\n",
    "        resnet_loss,\n",
    "        resnet_result,\n",
    "        training=False\n",
    "    )\n",
    "    \n",
    "    # Update scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save a copy just in case\n",
    "    if val_loss < best_loss:\n",
    "        torch.save(\n",
    "            resnet, \n",
    "            os.path.join(\n",
    "                os.path.abspath(\"\"),\n",
    "                f\"best_resnet_classifier_{best_count}_{int(time.time())}.pt\"\n",
    "            )\n",
    "        )\n",
    "        best_count += 1\n",
    "        best_loss = val_loss\n",
    "        \n",
    "    if epoch % SAVE_PERIOD == SAVE_PERIOD - 1:\n",
    "        torch.save(\n",
    "            resnet,\n",
    "            os.path.join(\n",
    "                os.path.abspath(\"\"),\n",
    "                f\"resnet_classifier_{epoch}_{int(time.time())}.pt\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Print the last loss calculated and the epoch\n",
    "    print(f\"\\nEpoch {epoch}: Training Loss: {train_loss}, \" \\\n",
    "          f\"Validation Loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7bf9a7-0dab-414d-bc04-0f2c9a82daf0",
   "metadata": {},
   "source": [
    "Let's view the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96da7756-7952-4b17-bfe2-2ab465476495",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "resnet_result.full_analysis(axs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde92e6b-ad83-48f2-902b-da0afe9cc9c0",
   "metadata": {},
   "source": [
    "#### Stability Trained\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83bea674-372f-46ff-8153-0caef4c42b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d009cd7c-cec2-4c08-8b22-f3c0c4fa59b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Triplet Ranking\n",
    "\n",
    "Next, let's train some triplet ranking models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b366d7f3-6e61-4dc4-9a49-38a60ee85b6a",
   "metadata": {},
   "source": [
    "#### Baseline\n",
    "\n",
    "Prepare data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b7ce7f81-c188-42e7-9082-117f2556a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(data_dir, \"train\")\n",
    "val_path = os.path.join(data_dir, \"val\")\n",
    "trd = TripletRankingDataset(train_path, 2, 2, transform=resnet_transform)\n",
    "trd_val = TripletRankingDataset(\n",
    "    val_path,\n",
    "    1,\n",
    "    1,\n",
    "    transform=resnet_transform,\n",
    "    training=False\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "tr_train_loader = torch.utils.data.DataLoader(trd, batch_size=batch_size,\n",
    "                                     num_workers=0, shuffle=True)\n",
    "tr_val_loader = torch.utils.data.DataLoader(trd_val, batch_size=batch_size,\n",
    "                                     num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331d938f-53c6-4f61-9e2b-d0a37b1fe9b4",
   "metadata": {},
   "source": [
    "Prepare new model that maps into a 64 dimensional encoding space. Note that this is less than the 200 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "df26a010-38a2-42c9-be99-c80c0b343749",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_encoder = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "for param in resnet_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "resnet_encoder.fc = nn.Linear(512, 64, bias=True) # 64-Dimensional Encoding\n",
    "\n",
    "resnet_triplet_ranking = TripletRanker(resnet_encoder).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69dc77d-6ad0-4976-985d-fbf1b46a8516",
   "metadata": {},
   "source": [
    "Training controllers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5a778ad3-0312-4f12-8a58-48cb8847caa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtr_optimizer = optim.SGD(resnet_triplet_ranking.parameters(), lr=0.01)\n",
    "rtr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    rtr_optimizer,\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    threshold=0.1\n",
    ")\n",
    "# May try OneCycleLR, annealers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2461a585-7d7d-45fa-9324-5556ccf70095",
   "metadata": {},
   "source": [
    "Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cc8db298-59a6-4925-896a-4ab0753ff472",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtr_result = TrainResult(resnet_triplet_ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6d7f05-c0e8-44c5-a18e-efcda7283164",
   "metadata": {},
   "source": [
    "Triplet loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "18086897-8081-4a74-92d3-d0eb5fd0c140",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtr_loss = nn.TripletMarginLoss(1.0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954ad413-8966-4cfc-8530-8061c4f12def",
   "metadata": {},
   "source": [
    "Training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d095a291-5ed8-46e5-b81b-42bd2079315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for some number of epochs\n",
    "RTR_EPOCH_COUNT = 20\n",
    "RTR_SAVE_PERIOD = 3\n",
    "\n",
    "loss = torch.Tensor([0])\n",
    "rtr_best_loss = float(\"inf\")\n",
    "rtr_best_count = 0\n",
    "for epoch in tqdm_variant(\n",
    "    range(RTR_EPOCH_COUNT),\n",
    "    desc=f\"Epoch\",\n",
    "    unit=\"epoch\",\n",
    "    disable=False\n",
    "):\n",
    "    # Debug learning rate\n",
    "    print(f\"Current Learning Rate: {rtr_optimizer.param_groups[0]['lr']}\")\n",
    "    \n",
    "    train_loss = triplet_epoch(\n",
    "        resnet_triplet_ranking,\n",
    "        tr_train_loader,\n",
    "        rtr_optimizer,\n",
    "        device,\n",
    "        rtr_loss,\n",
    "        rtr_result\n",
    "    )\n",
    "\n",
    "    val_loss = triplet_epoch(\n",
    "        resnet_triplet_ranking,\n",
    "        tr_val_loader,\n",
    "        rtr_optimizer,\n",
    "        device,\n",
    "        rtr_loss,\n",
    "        rtr_result,\n",
    "        training=False\n",
    "    )\n",
    "    \n",
    "    # Save a copy for safety\n",
    "    if val_loss < rtr_best_loss:\n",
    "        torch.save(\n",
    "            resnet_triplet_ranking, \n",
    "            os.path.join(\n",
    "                os.path.abspath(\"\"),\n",
    "                f\"best_rtr_{best_count}_{int(time.time()) % 1000000:06}.pt\"\n",
    "            )\n",
    "        )\n",
    "        rtr_best_count += 1\n",
    "        rtr_best_loss = val_loss\n",
    "        \n",
    "    if epoch % RTR_SAVE_PERIOD == RTR_SAVE_PERIOD - 1:\n",
    "        torch.save(\n",
    "            resnet_triplet_ranking,\n",
    "            os.path.join(\n",
    "                os.path.abspath(\"\"),\n",
    "                f\"rtr_{epoch}_{int(time.time()) % 1000000:06}.pt\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Print the last loss calculated and the epoch\n",
    "    print(f\"\\nEpoch {epoch}: Training Loss: {train_loss}, \" \\\n",
    "          f\"Validation Loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5941e0f-b182-4daf-b917-33d7283c414c",
   "metadata": {},
   "source": [
    "Let's double check gradient calculations went well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7b5313-cd95-4b07-a9c9-7c550874fc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_sanity_display(resnet_triplet_ranking, next(iter(tr_train_loader))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b139ab77-5f5b-406f-afcf-bd716ed4d2cf",
   "metadata": {},
   "source": [
    "#### Stability Trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c16049e-8833-4dc6-8c23-ce9e6f8c5af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4952048-8638-4874-b336-40d4a280ec07",
   "metadata": {},
   "source": [
    "## Experiments and Plot Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a6ef26d-e48d-4976-bd92-fbec7b31f604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
